{"main_page": "https://arxiv.org/abs/2404.00157", "pdf": "https://arxiv.org/pdf/2404.00157", "title": "Nonparametric Estimation of the Transition Density Function for  Diffusion Processes", "authors": "Fabienne Comte, Nicolas Marie", "subjects": "Statistics Theory (math.ST)", "abstract": "We assume that we observe $N$ independent copies of a diffusion process on a time interval $[0,2T]$. For a given time $t$, we estimate the transition density $p_t(x,y)$, namely the conditional density of $X_{t + s}$ given $X_s = x$, under conditions on the diffusion coefficients ensuring that this quantity exists. We use a least squares projection method on a product of finite dimensional spaces, prove risk bounds for the estimator and propose an anisotropic model selection method, relying on several reference norms. A simulation study illustrates the theoretical part for Ornstein-Uhlenbeck or square-root (Cox-Ingersoll-Ross) processes."}
{"main_page": "https://arxiv.org/abs/2404.00294", "pdf": "https://arxiv.org/pdf/2404.00294", "title": "Information divergences and likelihood ratios of Poisson processes and  point patterns", "authors": "Lasse Leskel\u00e4", "subjects": "Statistics Theory (math.ST); Information Theory (cs.IT); Probability (math.PR)", "abstract": "This article develops an analytical framework for studying information divergences and likelihood ratios associated with Poisson processes and point patterns on general measurable spaces. The main results include explicit analytical formulas for Kullback-Leibler divergences, R\\'enyi divergences, Hellinger distances, and likelihood ratios of the laws of Poisson point patterns in terms of their intensity measures. The general results yield similar formulas for inhomogeneous Poisson processes, compound Poisson processes, as well as spatial and marked Poisson point patterns. Additional results include simple characterisations of absolute continuity, mutual singularity, and the existence of common dominating measures. The analytical toolbox is based on Tsallis divergences of sigma-finite measures on abstract measurable spaces. The treatment is purely information-theoretic and free of any topological assumptions."}
{"main_page": "https://arxiv.org/abs/2404.00398", "pdf": "https://arxiv.org/pdf/2404.00398", "title": "Revisiting the region determined by Spearman's $\u03c1$ and Spearman's  footrule $\u03c6$", "authors": "Marco Tschimpke, Manuela Schreyer, Wolfgang Trutschnig", "subjects": "Statistics Theory (math.ST)", "abstract": "Kokol and Stopar ($2023$) recently studied the exact region $\\Omega_{\\phi,\\rho}$ determined by Spearman's footrule $\\phi$ and Spearman's $\\rho$ and derived a sharp lower, as well as a non-sharp upper bound for $\\rho$ given $\\phi$. Considering that the proofs for establishing these inequalities are novel and interesting, but technically quite involved we here provide alternative simpler proofs mainly building upon shuffles, symmetry, denseness and mass shifting. As a by-product of these proofs we derive several additional results on shuffle rearrangements and the interplay between diagonal copulas and shuffles which are of independent interest. Moreover we finally show that we can get closer to the (non-sharp) upper bound than established in the literature so far."}
{"main_page": "https://arxiv.org/abs/2404.00630", "pdf": "https://arxiv.org/pdf/2404.00630", "title": "Sobolev Calibration of Imperfect Computer Models", "authors": "Qingwen Zhang, Wenjia Wang", "subjects": "Statistics Theory (math.ST)", "abstract": "Calibration refers to the statistical estimation of unknown model parameters in computer experiments, such that computer experiments can match underlying physical systems. This work develops a new calibration method for imperfect computer models, Sobolev calibration, which can rule out calibration parameters that generate overfitting calibrated functions. We prove that the Sobolev calibration enjoys desired theoretical properties including fast convergence rate, asymptotic normality and semiparametric efficiency. We also demonstrate an interesting property that the Sobolev calibration can bridge the gap between two influential methods: $L_2$ calibration and Kennedy and O'Hagan's calibration. In addition to exploring the deterministic physical experiments, we theoretically justify that our method can transfer to the case when the physical process is indeed a Gaussian process, which follows the original idea of Kennedy and O'Hagan's. Numerical simulations as well as a real-world example illustrate the competitive performance of the proposed method."}
{"main_page": "https://arxiv.org/abs/2404.00753", "pdf": "https://arxiv.org/pdf/2404.00753", "title": "A compromise criterion for weighted least squares estimates", "authors": "Jordan Bryan, Haibo Zhou, Didong Li", "subjects": "Statistics Theory (math.ST); Methodology (stat.ME)", "abstract": "When independent errors in a linear model have non-identity covariance, the ordinary least squares estimate of the model coefficients is less efficient than the weighted least squares estimate. However, the practical application of weighted least squares is challenging due to its reliance on the unknown error covariance matrix. Although feasible weighted least squares estimates, which use an approximation of this matrix, often outperform the ordinary least squares estimate in terms of efficiency, this is not always the case. In some situations, feasible weighted least squares can be less efficient than ordinary least squares. This study identifies the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how a certain robust regression estimate behaves with respect to the linear model with normal errors of unequal variance."}
{"main_page": "https://arxiv.org/abs/2404.00784", "pdf": "https://arxiv.org/pdf/2404.00784", "title": "Estimating sample paths of Gauss-Markov processes from noisy data", "authors": "Benjamin Davies", "subjects": "Statistics Theory (math.ST); Econometrics (econ.EM); Probability (math.PR)", "abstract": "I derive the pointwise conditional means and variances of an arbitrary Gauss-Markov process, given noisy observations of points on a sample path. These moments depend on the process's mean and covariance functions, and on the conditional moments of the sampled points. I study the Brownian motion and bridge as special cases."}
{"main_page": "https://arxiv.org/abs/2404.00888", "pdf": "https://arxiv.org/pdf/2404.00888", "title": "Two step estimations via the Dantzig selector for models of stochastic  processes with high-dimensional parameters", "authors": "Kou Fujimori, Koji Tsukuda", "subjects": "Statistics Theory (math.ST)", "abstract": "We consider the sparse estimation for stochastic processes with possibly infinite-dimensional nuisance parameters, by using the Dantzig selector which is a sparse estimation method similar to $Z$-estimation. When a consistent estimator for a nuisance parameter is obtained, it is possible to construct an asymptotically normal estimator for the parameter of interest under appropriate conditions. Motivated by this fact, we establish the asymptotic behavior of the Dantzig selector for models of ergodic stochastic processes with high-dimensional parameters of interest and possibly infinite-dimensional nuisance parameters. Applications to ergodic time series models including integer-valued autoregressive models and ergodic diffusion processes are presented."}
{"main_page": "https://arxiv.org/abs/2404.00912", "pdf": "https://arxiv.org/pdf/2404.00912", "title": "Inference in Randomized Least Squares and PCA via Normality of Quadratic  Forms", "authors": "Leda Wang, Zhixiang Zhang, Edgar Dobriban", "subjects": "Statistics Theory (math.ST); Computation (stat.CO); Methodology (stat.ME); Machine Learning (stat.ML)", "abstract": "Randomized algorithms can be used to speed up the analysis of large datasets. In this paper, we develop a unified methodology for statistical inference via randomized sketching or projections in two of the most fundamental problems in multivariate statistical analysis: least squares and PCA. The methodology applies to fixed datasets -- i.e., is data-conditional -- and the only randomness is due to the randomized algorithm. We propose statistical inference methods for a broad range of sketching distributions, such as the subsampled randomized Hadamard transform (SRHT), Sparse Sign Embeddings (SSE) and CountSketch, sketching matrices with i.i.d. entries, and uniform subsampling. To our knowledge, no comparable methods are available for SSE and for SRHT in PCA. Our novel theoretical approach rests on showing the asymptotic normality of certain quadratic forms. As a contribution of broader interest, we show central limit theorems for quadratic forms of the SRHT, relying on a novel proof via a dyadic expansion that leverages the recursive structure of the Hadamard transform. Numerical experiments using both synthetic and empirical datasets support the efficacy of our methods, and in particular suggest that sketching methods can have better computation-estimation tradeoffs than recently proposed optimal subsampling methods."}
{"main_page": "https://arxiv.org/abs/2404.01233", "pdf": "https://arxiv.org/pdf/2404.01233", "title": "Optimal Ridge Regularization for Out-of-Distribution Prediction", "authors": "Pratik Patil, Jin-Hong Du, Ryan J. Tibshirani", "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels."}
{"main_page": "https://arxiv.org/abs/2404.01245", "pdf": "https://arxiv.org/pdf/2404.01245", "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,  Detection Efficiency and Optimal Rules", "authors": "Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su", "subjects": "Statistics Theory (math.ST); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments."}

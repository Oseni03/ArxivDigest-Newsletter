{"main_page": "https://arxiv.org/abs/2404.00011", "pdf": "https://arxiv.org/pdf/2404.00011", "title": "A novel interface for adversarial trivia question-writing", "authors": "Jason Liu", "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL)", "abstract": "A critical component when developing question-answering AIs is an adversarial dataset that challenges models to adapt to the complex syntax and reasoning underlying our natural language. Present techniques for procedurally generating adversarial texts are not robust enough for training on complex tasks such as answering multi-sentence trivia questions. We instead turn to human-generated data by introducing an interface for collecting adversarial human-written trivia questions. Our interface is aimed towards question writers and players of Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions consist of a sequence of clues of decreasing difficulty. To incentivize usage, a suite of machine learning-based tools in our interface assist humans in writing questions that are more challenging to answer for Quiz Bowl players and computers alike. Not only does our interface gather training data for the groundbreaking Quiz Bowl AI project QANTA, but it is also a proof-of-concept of future adversarial data collection for question-answering systems. The results of performance-testing our interface with ten originally-composed questions indicate that, despite some flaws, our interface's novel question-writing features as well as its real-time exposure of useful responses from our machine models could facilitate and enhance the collection of adversarial questions."}
{"main_page": "https://arxiv.org/abs/2404.00016", "pdf": "https://arxiv.org/pdf/2404.00016", "title": "SOMson -- Sonification of Multidimensional Data in Kohonen Maps", "authors": "Simon Linke, Tim Ziemer", "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "abstract": "Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that visualize a high-dimensional feature space on a low-dimensional map. While SOMs are an excellent tool for data examination and exploration, they inherently cause a loss of detail. Visualizations of the underlying data do not integrate well and, therefore, fail to provide an overall picture. Consequently, we suggest SOMson, an interactive sonification of the underlying data, as a data augmentation technique. The sonification increases the amount of information provided simultaneously by the SOM. Instead of a user study, we present an interactive online example, so readers can explore SOMson themselves. Its strengths, weaknesses, and prospects are discussed."}
{"main_page": "https://arxiv.org/abs/2404.00018", "pdf": "https://arxiv.org/pdf/2404.00018", "title": "Can AI Outperform Human Experts in Creating Social Media Creatives?", "authors": "Eunkyung Park, Raymond K. Wong, Junbum Kwon", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)", "abstract": "Artificial Intelligence has outperformed human experts in functional tasks such as chess and baduk. How about creative tasks? This paper evaluates AI's capability in the creative domain compared to human experts, which little research has been conducted so far. We propose a novel Prompt-for-Prompt to generate social media creatives via prompt augmentation by Large Language Models. We take the most popular Instagram posts (with the biggest number of like clicks) in top brands' Instagram accounts to create social media creatives. We give GPT 4 several prompt instructions with text descriptions to generate the most effective prompts for cutting-edge text-to-image generators: Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost AI's abilities by adding objectives, engagement strategy, lighting and brand consistency for social media image creation. We conduct an extensive human evaluation experiment, and find that AI excels human experts, and Midjourney is better than the other text-to-image generators. Surprisingly, unlike conventional wisdom in the social media industry, prompt instruction including eye-catching shows much poorer performance than those including natural. Regarding the type of creatives, AI improves creatives with animals or products but less with real people. Also, AI improves creatives with short text descriptions more than with long text descriptions, because there is more room for AI to augment prompts with shorter descriptions."}
{"main_page": "https://arxiv.org/abs/2404.00019", "pdf": "https://arxiv.org/pdf/2404.00019", "title": "Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review  and Research Roadmap", "authors": "Sule Tekkesinoglu, Azra Habibovic, Lars Kunze", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)", "abstract": "Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanations, (ii) communicating human-friendly explanations, and (iv) continuous learning. Our roadmap is underpinned by principles of responsible research and innovation, emphasising the significance of diverse explanation requirements. To effectively tackle the challenges associated with implementing explainable AV systems, we have delineated various research directions, including the development of privacy-preserving data integration, ethical frameworks, real-time analytics, human-centric interaction design, and enhanced cross-disciplinary collaborations. By exploring these research directions, the study aims to guide the development and deployment of explainable AVs, informed by a holistic understanding of user needs, technological advancements, regulatory compliance, and ethical considerations, thereby ensuring safer and more trustworthy autonomous driving experiences."}
{"main_page": "https://arxiv.org/abs/2404.00021", "pdf": "https://arxiv.org/pdf/2404.00021", "title": "Evaluatology: The Science and Engineering of Evaluation", "authors": "Jianfeng Zhan, Lei Wang, Wanling Gao, Hongxiao Li, Chenxi Wang, Yunyou Huang, Yatao Li, Zhengxin Yang, Guoxin Kang, Chunjie Luo, Hainan Ye, Shaopeng Dai, Zhifei Zhang", "subjects": "Human-Computer Interaction (cs.HC); Computational Engineering, Finance, and Science (cs.CE); Computers and Society (cs.CY); Performance (cs.PF)", "abstract": "Evaluation is a crucial aspect of human existence and plays a vital role in various fields. However, it is often approached in an empirical and ad-hoc manner, lacking consensus on universal concepts, terminologies, theories, and methodologies. This lack of agreement has significant repercussions. This article aims to formally introduce the discipline of evaluatology, which encompasses the science and engineering of evaluation. We propose a universal framework for evaluation, encompassing concepts, terminologies, theories, and methodologies that can be applied across various disciplines. Our research reveals that the essence of evaluation lies in conducting experiments that intentionally apply a well-defined evaluation condition to diverse subjects and infer the impact of different subjects by measuring and/or testing. Derived from the essence of evaluation, we propose five axioms focusing on key aspects of evaluation outcomes as the foundational evaluation theory. These axioms serve as the bedrock upon which we build universal evaluation theories and methodologies. When evaluating a single subject, it is crucial to create evaluation conditions with different levels of equivalency. By applying these conditions to diverse subjects, we can establish reference evaluation models. These models allow us to alter a single independent variable at a time while keeping all other variables as controls. When evaluating complex scenarios, the key lies in establishing a series of evaluation models that maintain transitivity. Building upon the science of evaluation, we propose a formal definition of a benchmark as a simplified and sampled evaluation condition that guarantees different levels of equivalency. This concept serves as the cornerstone for a universal benchmark-based engineering approach to evaluation across various disciplines, which we refer to as benchmarkology."}
{"main_page": "https://arxiv.org/abs/2404.00022", "pdf": "https://arxiv.org/pdf/2404.00022", "title": "Analysing and Organising Human Communications for AI Fairness-Related  Decisions: Use Cases from the Public Sector", "authors": "Mirthe Dankloff, Vanja Skoric, Giovanni Sileno, Sennay Ghebreab, Jacco Van Ossenbruggen, Emma Beauxis-Aussalet", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "AI algorithms used in the public sector, e.g., for allocating social benefits or predicting fraud, often involve multiple public and private stakeholders at various phases of the algorithm's life-cycle. Communication issues between these diverse stakeholders can lead to misinterpretation and misuse of algorithms. We investigate the communication processes for AI fairness-related decisions by conducting interviews with practitioners working on algorithmic systems in the public sector. By applying qualitative coding analysis, we identify key elements of communication processes that underlie fairness-related human decisions. We analyze the division of roles, tasks, skills, and challenges perceived by stakeholders. We formalize the underlying communication issues within a conceptual framework that i. represents the communication patterns ii. outlines missing elements, such as actors who miss skills for their tasks. The framework is used for describing and analyzing key organizational issues for fairness-related decisions. Three general patterns emerge from the analysis: 1. Policy-makers, civil servants, and domain experts are less involved compared to developers throughout a system's life-cycle. This leads to developers taking on extra roles such as advisor, while they potentially miss the required skills and guidance from domain experts. 2. End-users and policy-makers often lack the technical skills to interpret a system's limitations, and rely on developer roles for making decisions concerning fairness issues. 3. Citizens are structurally absent throughout a system's life-cycle, which may lead to decisions that do not include relevant considerations from impacted stakeholders."}
{"main_page": "https://arxiv.org/abs/2404.00025", "pdf": "https://arxiv.org/pdf/2404.00025", "title": "Understanding Physical Breakdowns in Virtual Reality", "authors": "Wen-Jie Tseng", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Virtual Reality (VR) moves away from well-controlled laboratory environments into public and personal spaces. As users are visually disconnected from the physical environment, interacting in an uncontrolled space frequently leads to collisions and raises safety concerns. In my thesis, I investigate this phenomenon which I define as the physical breakdown in VR. The goal is to understand the reasons for physical breakdowns, provide solutions, and explore future mechanisms that could perpetuate safety risks. First, I explored the reasons for physical breakdowns by investigating how people interact with the current VR safety mechanism (e.g., Oculus Guardian). Results show one reason for breaking out of the safety boundary is when interacting with large motions (e.g., swinging arms), the user does not have enough time to react although they see the safety boundary. I proposed a solution, FingerMapper, that maps small-scale finger motions onto virtual arms and hands to enable whole-body virtual arm motions in VR to avoid physical breakdowns. To demonstrate future safety risks, I explored the malicious use of perceptual manipulations (e.g., redirection techniques) in VR, which could deliberately create physical breakdowns without users noticing. Results indicate further open challenges about the cognitive process of how users comprehend their physical environment when they are blindfolded in VR."}
{"main_page": "https://arxiv.org/abs/2404.00026", "pdf": "https://arxiv.org/pdf/2404.00026", "title": "Ink and Individuality: Crafting a Personalised Narrative in the Age of  LLMs", "authors": "Azmine Toushik Wasi, Raima Islam, Rafia Islam", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)", "abstract": "Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality."}
{"main_page": "https://arxiv.org/abs/2404.00027", "pdf": "https://arxiv.org/pdf/2404.00027", "title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership  and Reasoning", "authors": "Azmine Toushik Wasi, Rafia Islam, Raima Islam", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)", "abstract": "Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems."}
{"main_page": "https://arxiv.org/abs/2404.00029", "pdf": "https://arxiv.org/pdf/2404.00029", "title": "Complementarity in Human-AI Collaboration: Concept, Sources, and  Evidence", "authors": "Patrick Hemmer, Max Schemmer, Niklas K\u00fchl, Michael V\u00f6ssing, Gerhard Satzger", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry as a source and, in a real estate appraisal use case, demonstrate that humans can leverage unique contextual information to achieve CTP. In the second study, we focus on capability asymmetry as an alternative source, demonstrating how heterogeneous capabilities can help achieve CTP. Our work provides researchers with a theoretical foundation of complementarity in human-AI decision-making and demonstrates that leveraging sources of complementarity potential constitutes a viable pathway toward effective human-AI collaboration."}
{"main_page": "https://arxiv.org/abs/2404.00030", "pdf": "https://arxiv.org/pdf/2404.00030", "title": "Visualization of Unstructured Sports Data -- An Example of Cricket Short  Text Commentary", "authors": "Swarup Ranjan Behera, Vijaya V Saradhi", "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "abstract": "Sports visualization focuses on the use of structured data, such as box-score data and tracking data. Unstructured data sources pertaining to sports are available in various places such as blogs, social media posts, and online news articles. Sports visualization methods either not fully exploited the information present in these sources or the proposed visualizations through the use of these sources did not augment to the body of sports visualization methods. We propose the use of unstructured data, namely cricket short text commentary for visualization. The short text commentary data is used for constructing individual player's strength rules and weakness rules. A computationally feasible definition for player's strength rule and weakness rule is proposed. A visualization method for the constructed rules is presented. In addition, players having similar strength rules or weakness rules is computed and visualized. We demonstrate the usefulness of short text commentary in visualization by analyzing the strengths and weaknesses of cricket players using more than one million text commentaries. We validate the constructed rules through two validation methods. The collected data, source code, and obtained results on more than 500 players are made publicly available."}
{"main_page": "https://arxiv.org/abs/2404.00031", "pdf": "https://arxiv.org/pdf/2404.00031", "title": "Towards gaze-independent c-VEP BCI: A pilot study", "authors": "S. Narayanan, S. Ahmadi, P. Desain, J. Thielen", "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "abstract": "A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promising feasibility of utilizing the c-VEP protocol for gaze-independent BCIs that use covert spatial attention when both stimuli flash simultaneously."}
{"main_page": "https://arxiv.org/abs/2404.00032", "pdf": "https://arxiv.org/pdf/2404.00032", "title": "Deployment of Deep Learning Model in Real World Clinical Setting: A Case  Study in Obstetric Ultrasound", "authors": "Chun Kit Wong, Mary Ngo, Manxi Lin, Zahra Bashir, Amihai Heen, Morten Bo S\u00f8ndergaard Svendsen, Martin Gr\u00f8nneb\u00e6k Tolsgaard, Anders Nymark Christensen, Aasa Feragen", "subjects": "Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)", "abstract": "Despite the rapid development of AI models in medical image analysis, their validation in real-world clinical settings remains limited. To address this, we introduce a generic framework designed for deploying image-based AI models in such settings. Using this framework, we deployed a trained model for fetal ultrasound standard plane detection, and evaluated it in real-time sessions with both novice and expert users. Feedback from these sessions revealed that while the model offers potential benefits to medical practitioners, the need for navigational guidance was identified as a key area for improvement. These findings underscore the importance of early deployment of AI models in real-world settings, leading to insights that can guide the refinement of the model and system based on actual user feedback."}
{"main_page": "https://arxiv.org/abs/2404.00033", "pdf": "https://arxiv.org/pdf/2404.00033", "title": "The Hall of Singularity: VR Experience of Prophecy by AI", "authors": "Jisu Kim, Kirak Kim", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "\"The Hall of Singularity\" is an immersive art that creates personalized experiences of receiving prophecies from an AI deity through an integration of Artificial Intelligence (AI) and Virtual Reality (VR). As a metaphor for the mythologizing of AI in our society, \"The Hall of Singularity\" offers an immersive quasi-religious experience where individuals can encounter an AI that has the power to make prophecies. This journey enables users to experience and imagine a world with an omnipotent AI deity."}
{"main_page": "https://arxiv.org/abs/2404.00043", "pdf": "https://arxiv.org/pdf/2404.00043", "title": "Improve accessibility for Low Vision and Blind people using Machine  Learning and Computer Vision", "authors": "Jasur Shukurov", "subjects": "Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting objects around the user, and providing audio feedback about those objects. It also includes providing the description of the objects and their location, and giving haptic feedback if the user is too close to an object. The last feature is currency detection which provides a total amount of currency value to the user via the camera."}
{"main_page": "https://arxiv.org/abs/2404.00047", "pdf": "https://arxiv.org/pdf/2404.00047", "title": "Guidelines for Public and Patient Involvement in Neurotechnology in the  United Kingdom", "authors": "Amparo Guemes Gonzalez, Tiago da Silva Costa, Tamar Makin", "subjects": "Human-Computer Interaction (cs.HC); Computers and Society (cs.CY)", "abstract": "Neurotechnologies are increasingly becoming integrated in our everyday lives, our bodies and minds. As the popularity and impact of neurotech grows, so does our responsibility to ensure we understand its particular ethical and societal implications. Enabling end-users and other stakeholders to participate in the development of neurotechnology, even at its earliest stages of conception, will help us better navigate our design around these serious considerations, and deliver more impactful technologies. There are many different terms and frameworks to articulate the concept of involving end users in the technology development lifecycle: 'Public and Patient Involvement and Engagement' (PPIE), 'lived experience', 'co-design', 'co-production'. What is lacking are clear guidelines for implementing a robust PPIE process in neurotechnology. While general advice is available online, it is down to individuals (and their funders) to carve up their own approach to meaningful involvement. Here we present guidance for UK-based researchers and engineers to conduct PPI for neurotechnology. The overall aim is the establishment of gold-standard PPIE methodologies in the neurotechnology space that bring patient and public insights at the forefront of our scientific inquiry and product development."}
{"main_page": "https://arxiv.org/abs/2404.00048", "pdf": "https://arxiv.org/pdf/2404.00048", "title": "SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System  For Hyperspectral Classification Mapping with Depth Information for In-Vivo  Surgical Procedures", "authors": "Jaime Sancho, Manuel Villa, Miguel Chavarr\u00edas, Eduardo Juarez, Alfonso Lagares, C\u00e9sar Sanz", "subjects": "Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scene at the same time it is captured and processed, improving the visualization and hence effectiveness of the HS technology to delimit tumors. The whole system has been verified in real brain tumor resection operations."}
{"main_page": "https://arxiv.org/abs/2404.00049", "pdf": "https://arxiv.org/pdf/2404.00049", "title": "Web-based Interactive Narratives to Present Business Processes Models", "authors": "M\u00e1rcio Rocha Ferreira, Tadeu Moreira de Classe, Sean Wolfgand Matsui Siqueira", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Interactive narratives offer a novel approach to presenting business process models, making them more accessible and collaborative. These narratives create a hyper-textual environment that facilitates knowledge exchange and comprehension for ordinary individuals. However, designing such narratives is complex, as business process modelers must accurately identify and translate the graphic elements of a process model into dynamic narrative elements. This research paper introduces the Scripting Your Process (SYP) method, which provides a systematic approach to designing interactive narratives based on business process models. Following the principles of Design Science Research (DSR), a quasi-experimental study demonstrates and evaluates the SYP method. The results show that the SYP method successfully achieves its objective, contributing to the systematic design of interactive narratives derived from business process models. Consequently, individuals who are not experts in business process management can understand these processes in an engaging and gameful manner."}
{"main_page": "https://arxiv.org/abs/2404.00054", "pdf": "https://arxiv.org/pdf/2404.00054", "title": "Choreographing the Digital Canvas: A Machine Learning Approach to  Artistic Performance", "authors": "Siyuan Peng, Kate Ladenheim, Snehesh Shrestha, Cornelia Ferm\u00fcller", "subjects": "Human-Computer Interaction (cs.HC); Graphics (cs.GR); Machine Learning (cs.LG)", "abstract": "This paper introduces the concept of a design tool for artistic performances based on attribute descriptions. To do so, we used a specific performance of falling actions. The platform integrates a novel machine-learning (ML) model with an interactive interface to generate and visualize artistic movements. Our approach's core is a cyclic Attribute-Conditioned Variational Autoencoder (AC-VAE) model developed to address the challenge of capturing and generating realistic 3D human body motions from motion capture (MoCap) data. We created a unique dataset focused on the dynamics of falling movements, characterized by a new ontology that divides motion into three distinct phases: Impact, Glitch, and Fall. The ML model's innovation lies in its ability to learn these phases separately. It is achieved by applying comprehensive data augmentation techniques and an initial pose loss function to generate natural and plausible motion. Our web-based interface provides an intuitive platform for artists to engage with this technology, offering fine-grained control over motion attributes and interactive visualization tools, including a 360-degree view and a dynamic timeline for playback manipulation. Our research paves the way for a future where technology amplifies the creative potential of human expression, making sophisticated motion generation accessible to a wider artistic community."}
{"main_page": "https://arxiv.org/abs/2404.00057", "pdf": "https://arxiv.org/pdf/2404.00057", "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud", "authors": "Hongyu H\u00e8", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Operating Systems (cs.OS)", "abstract": "Operating systems (OSes) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. However, despite their enduring importance, the fundamental design objectives of OSes have seen minimal evolution over decades. Traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. The lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ML). Today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional OSes like Linux and iOS, especially with the emergence of specialized hardware featuring heterogeneous components. Furthermore, the rise of large language models (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and software development paradigms. While existing literature predominantly focuses on leveraging ML methods for system optimization or accelerating ML workloads, there is a significant gap in addressing personalized user experiences at the OS level. To tackle this challenge, this work proposes PerOS, a personalized OS ingrained with LLM capabilities. PerOS aims to provide tailored user experiences while safeguarding privacy and personal data through declarative interfaces, self-adaptive kernels, and secure data management in a scalable cloud-centric architecture; therein lies the main research question of this work: How can we develop intelligent, secure, and scalable OSes that deliver personalized experiences to thousands of users?"}
{"main_page": "https://arxiv.org/abs/2404.00061", "pdf": "https://arxiv.org/pdf/2404.00061", "title": "ERIOS: Co-construction of a Dynamic Temporal Visualization Tool in the  Electronic Health Record", "authors": "Louise Robert, Quentin Luzurier, Ana\u00efs Velcker, Emma Mathieu, Loic Fontaine, David Morquin", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "ERIOS, is a collaborative project between Dedalus, a health software company, Montpellier University Hospital Center (CHU), and the University of Montpellier. This initiative aims to incorporate research and development (R\\&D) directly within the hospital, focusing on co-creating components of the Electronic Health Record (EHR) alongside end-users. The project was initiated with two initial use cases, which led to the development of components for dynamic temporal visualization, now integrated into specific dashboards. The application of academic recommendations regarding user engagement methodology and human-computer interactions significantly enhanced our ability to meet user needs."}
{"main_page": "https://arxiv.org/abs/2404.00131", "pdf": "https://arxiv.org/pdf/2404.00131", "title": "Give Text A Chance: Advocating for Equal Consideration for Language and  Visualization", "authors": "Chase Stokes, Marti A. Hearst", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Visualization research tends to de-emphasize consideration of the textual context in which its images are placed. We argue that visualization research should consider textual representations as a primary alternative to visual options when assessing designs, and when assessing designs, equal attention should be given to the construction of the language as to the visualizations. We also call for a consideration of readability when integrating visualizations with written text. In highlighting these points, visualization research would be elevated in efficacy and demonstrate thorough accounting for viewers' needs and responses."}
{"main_page": "https://arxiv.org/abs/2404.00161", "pdf": "https://arxiv.org/pdf/2404.00161", "title": "Circle Back Next Week: The Effect of Meeting-Free Weeks on Distributed  Workers' Unstructured Time and Attention Negotiation", "authors": "Sharon Ferguson, Michael Massimi", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "While distributed workers rely on scheduled meetings for coordination and collaboration, these meetings can also challenge their ability to focus. Protecting worker focus has been addressed from a technical perspective, but companies are now attempting organizational interventions, such as meeting-free weeks. Recognizing distributed collaboration as a sociotechnical challenge, we first present an interview study with distributed workers participating in meeting-free weeks at an enterprise software company. We identify three orientations workers exhibit during these weeks: Focus, Collaborative, and Time-Bound, each with varying levels and use of unstructured time. These different orientations result in challenges in attention negotiation, which may be suited for technical interventions. This motivated a follow-up study investigating attention negotiation and the compensating mechanisms workers developed during meeting-free weeks. Our framework identified tensions between the attention-getting and attention-delegation strategies. We extend past work to show how workers adapt their virtual collaboration mechanisms in response to organizational interventions"}
{"main_page": "https://arxiv.org/abs/2404.00171", "pdf": "https://arxiv.org/pdf/2404.00171", "title": "No Risk, No Reward: Towards An Automated Measure of Psychological Safety  from Online Communication", "authors": "Sharon Ferguson, Georgia Van de Zande, Alison Olechowski", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "The data created from virtual communication platforms presents the opportunity to explore automated measures for monitoring team performance. In this work, we explore one important characteristic of successful teams - Psychological Safety - or the belief that a team is safe for interpersonal risk-taking. To move towards an automated measure of this phenomenon, we derive virtual communication characteristics and message keywords related to elements of Psychological Safety from the literature. Using a mixed methods approach, we investigate whether these characteristics are present in the Slack messages from two design teams - one high in Psychological Safety, and one low. We find that some usage characteristics, such as replies, reactions, and user mentions, might be promising metrics to indicate higher levels of Psychological Safety, while simple keyword searches may not be nuanced enough. We present the first step towards the automated detection of this important, yet complex, team characteristic."}
{"main_page": "https://arxiv.org/abs/2404.00192", "pdf": "https://arxiv.org/pdf/2404.00192", "title": "Tools and Tasks in Sensemaking: A Visual Accessibility Perspective", "authors": "Yichun Zhao, Miguel A. Nacenta", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Our previous interview study explores the needs and uses of diagrammatic information by the Blind and Low Vision (BLV) community, resulting in a framework called the Ladder of Diagram Access. The framework outlines five levels of information access when interacting with a diagram. In this paper, we connect this framework to include the global activity of sensemaking and discuss its (in)accessibility to the BLV demographic. We also discuss the integration of this framework into the sensemaking process and explore the current sensemaking practices and strategies employed by the BLV community, the challenges they face at different levels of the ladder, and potential solutions to enhance inclusivity towards a data-driven workforce."}
{"main_page": "https://arxiv.org/abs/2404.00300", "pdf": "https://arxiv.org/pdf/2404.00300", "title": "Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset  Modulation", "authors": "Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players' mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention's effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments."}
{"main_page": "https://arxiv.org/abs/2404.00333", "pdf": "https://arxiv.org/pdf/2404.00333", "title": "On Task and in Sync: Examining the Relationship between Gaze Synchrony  and Self-Reported Attention During Video Lecture Learning", "authors": "Babette B\u00fchler, Efe Bozkir, Hannah Deininger, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Successful learning depends on learners' ability to sustain attention, which is particularly challenging in online education due to limited teacher interaction. A potential indicator for attention is gaze synchrony, demonstrating predictive power for learning achievements in video-based learning in controlled experiments focusing on manipulating attention. This study (N=84) examines the relationship between gaze synchronization and self-reported attention of learners, using experience sampling, during realistic online video learning. Gaze synchrony was assessed through Kullback-Leibler Divergence of gaze density maps and MultiMatch algorithm scanpath comparisons. Results indicated significantly higher gaze synchronization in attentive participants for both measures and self-reported attention significantly predicted post-test scores. In contrast, synchrony measures did not correlate with learning outcomes. While supporting the hypothesis that attentive learners exhibit similar eye movements, the direct use of synchrony as an attention indicator poses challenges, requiring further research on the interplay of attention, gaze synchrony, and video content type."}
{"main_page": "https://arxiv.org/abs/2404.00392", "pdf": "https://arxiv.org/pdf/2404.00392", "title": "Designing a User-centric Framework for Information Quality Ranking of  Large-scale Street View Images", "authors": "Tahiya Chowdhury, Ilan Mandel, Jorge Ortiz, Wendy Ju", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Street view imagery (SVI), largely captured via outfitted fleets or mounted dashcams in consumer vehicles is a rapidly growing source of geospatial data used in urban sensing and development. These datasets are often collected opportunistically, are massive in size, and vary in quality which limits the scope and extent of their use in urban planning. Thus far there has not been much work to identify the obstacles experienced and tools needed by the users of such datasets. This severely limits the opportunities of using emerging street view images in supporting novel research questions that can improve the quality of urban life. This work includes a formative interview study with 5 expert users of large-scale street view datasets from academia, urban planning, and related professions which identifies novel use cases, challenges, and opportunities to increase the utility of these datasets. Based on the user findings, we present a framework to evaluate the quality of information for street images across three attributes (spatial, temporal, and content) that stakeholders can utilize for estimating the value of a dataset, and to improve it over time for their respective use case. We then present a case study using novel street view images where we evaluate our framework and present practical use cases for users. We discuss the implications for designing future systems to support the collection and use of street view data to assist in sensing and planning the urban environment."}
{"main_page": "https://arxiv.org/abs/2404.00405", "pdf": "https://arxiv.org/pdf/2404.00405", "title": "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration", "authors": "Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the \"5W1H\" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction."}
{"main_page": "https://arxiv.org/abs/2404.00431", "pdf": "https://arxiv.org/pdf/2404.00431", "title": "Visualizing Routes with AI-Discovered Street-View Patterns", "authors": "Tsung Heng Wu, Md Amiruzzaman, Ye Zhao, Deepshikha Bhati, Jing Yang", "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "abstract": "Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively and interactively explore multiple routes. Furthermore, we conducted a user study to assess the usefulness and utility of VivaRoutes."}
{"main_page": "https://arxiv.org/abs/2404.00487", "pdf": "https://arxiv.org/pdf/2404.00487", "title": "Contextual AI Journaling: Integrating LLM and Time Series Behavioral  Sensing Technology to Promote Self-Reflection and Well-being using the  MindScape App", "authors": "Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI."}
{"main_page": "https://arxiv.org/abs/2404.00526", "pdf": "https://arxiv.org/pdf/2404.00526", "title": "The Emotional Impact of Game Duration: A Framework for Understanding  Player Emotions in Extended Gameplay Sessions", "authors": "Anoop Kumar, Suresh Dodda, Navin Kamuni, Venkata Sai Mahesh Vuppalapati", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According to the results, it was recommended that in order to lessen the potential emotional impact that playing computer and video games may have in the future, game producers should think about creating shorter, entertaining games."}
{"main_page": "https://arxiv.org/abs/2404.00573", "pdf": "https://arxiv.org/pdf/2404.00573", "title": "\"My agent understands me better\": Integrating Dynamic Human-like Memory  Recall and Consolidation in LLM-Based Agents", "authors": "Yuki Hou, Haruki Tamoto, Homei Miyashita", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences."}
{"main_page": "https://arxiv.org/abs/2404.00634", "pdf": "https://arxiv.org/pdf/2404.00634", "title": "Designing Human-AI Systems: Anthropomorphism and Framing Bias on  Human-AI Collaboration", "authors": "Samuel Aleksander S\u00e1nchez Olszewski", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered recommendation tool. The tool was manipulated to have either human-like or robot-like characteristics and presented its recommendations in either positive or negative frames. The results revealed that the framing of AI's recommendations had no significant influence on subjects' decisions. In contrast, anthropomorphism significantly affected subjects' agreement with AI recommendations. Contrary to expectations, subjects were less likely to agree with the AI if it had human-like characteristics. These findings demonstrate that cognitive biases can impact human-AI collaboration and highlight the need for tailored approaches to AI product design, rather than a single, universal solution."}
{"main_page": "https://arxiv.org/abs/2404.00938", "pdf": "https://arxiv.org/pdf/2404.00938", "title": "How Can Large Language Models Enable Better Socially Assistive  Human-Robot Interaction: A Brief Survey", "authors": "Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matari\u0107", "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "abstract": "Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies."}
{"main_page": "https://arxiv.org/abs/2404.01063", "pdf": "https://arxiv.org/pdf/2404.01063", "title": "Chat Modeling: Natural Language-based Procedural Modeling of Biological  Structures without Training", "authors": "Donggang Jia, Yunhai Wang, Ivan Viola", "subjects": "Human-Computer Interaction (cs.HC); Graphics (cs.GR)", "abstract": "3D modeling of biological structures is an inherently complex process, necessitating both biological and geometric understanding. Additionally, the complexity of user interfaces of 3D modeling tools and the associated steep learning curve further exacerbate the difficulty of authoring a 3D model. In this paper, we introduce a novel framework to address the challenge of using 3D modeling software by converting users' textual inputs into modeling actions within an interactive procedural modeling system. The framework incorporates a code generator of a novel code format and a corresponding code interpreter. The major technical innovation includes the user-refinement mechanism that captures the degree of user dissatisfaction with the modeling outcome, offers an interactive revision, and leverages this feedback for future improved 3D modeling. This entire framework is powered by large language models and eliminates the need for a traditional training process. We develop a prototype tool named Chat Modeling, offering both automatic and step-by-step 3D modeling approaches. Our evaluation of the framework with structural biologists highlights the potential of our approach being utilized in their scientific workflows. All supplemental materials are available at https://osf.io/x4qb7/."}

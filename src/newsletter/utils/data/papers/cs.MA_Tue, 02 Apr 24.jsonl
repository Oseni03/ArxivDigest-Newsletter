{"main_page": "https://arxiv.org/abs/2404.00622", "pdf": "https://arxiv.org/pdf/2404.00622", "title": "OpenMines: A Light and Comprehensive Mining Simulation Environment for  Truck Dispatching", "authors": "Shi Meng, Bin Tian, Xiaotong Zhang, Shuangying Qi, Caiji Zhang, Qiang Zhang", "subjects": "Multiagent Systems (cs.MA); Systems and Control (eess.SY)", "abstract": "Mine fleet management algorithms can significantly reduce operational costs and enhance productivity in mining systems. Most current fleet management algorithms are evaluated based on self-implemented or proprietary simulation environments, posing challenges for replication and comparison. This paper models the simulation environment for mine fleet management from a complex systems perspective. Building upon previous work, we introduce probabilistic, user-defined events for random event simulation and implement various evaluation metrics and baselines, effectively reflecting the robustness of fleet management algorithms against unforeseen incidents. We present ``OpenMines'', an open-source framework encompassing the entire process of mine system modeling, algorithm development, and evaluation, facilitating future algorithm comparison and replication in the field. Code is available in https://github.com/370025263/openmines."}
{"main_page": "https://arxiv.org/abs/2404.01114", "pdf": "https://arxiv.org/pdf/2404.01114", "title": "A CRISP-DM-based Methodology for Assessing Agent-based Simulation Models  using Process Mining", "authors": "Rob H. Bemthuis, Ruben R. Govers, Amin Asadi", "subjects": "Multiagent Systems (cs.MA)", "abstract": "Agent-based simulation (ABS) models are potent tools for analyzing complex systems. However, understanding and validating ABS models can be a significant challenge. To address this challenge, cutting-edge data-driven techniques offer sophisticated capabilities for analyzing the outcomes of ABS models. One such technique is process mining, which encompasses a range of methods for discovering, monitoring, and enhancing processes by extracting knowledge from event logs. However, applying process mining to event logs derived from ABSs is not trivial, and deriving meaningful insights from the resulting process models adds an additional layer of complexity. Although process mining is invaluable in extracting insights from ABS models, there is a lack of comprehensive methodological guidance for its application in ABS evaluation in the research landscape. In this paper, we propose a methodology, based on the CRoss-Industry Standard Process for Data Mining (CRISP-DM) methodology, to assess ABS models using process mining techniques. We incorporate process mining techniques into the stages of the CRISP-DM methodology, facilitating the analysis of ABS model behaviors and their underlying processes. We demonstrate our methodology using an established agent-based model, Schelling model of segregation. Our results show that our proposed methodology can effectively assess ABS models through produced event logs, potentially paving the way for enhanced agent-based model validity and more insightful decision-making."}
{"main_page": "https://arxiv.org/abs/2404.01131", "pdf": "https://arxiv.org/pdf/2404.01131", "title": "GOV-REK: Governed Reward Engineering Kernels for Designing Robust  Multi-Agent Reinforcement Learning Systems", "authors": "Ashish Rana, Michael Oesterle, Jannik Brinkmann", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "abstract": "For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner. Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems."}

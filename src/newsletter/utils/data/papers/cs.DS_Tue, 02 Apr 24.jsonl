{"main_page": "https://arxiv.org/abs/2404.00041", "pdf": "https://arxiv.org/pdf/2404.00041", "title": "On contention resolution for the hypergraph matching, knapsack, and  $k$-column sparse packing problems", "authors": "Ivan Sergeev", "subjects": "Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)", "abstract": "The contention resolution framework is a versatile rounding technique used as a part of the relaxation and rounding approach for solving constrained submodular function maximization problems. We apply this framework to the hypergraph matching, knapsack, and $k$-column sparse packing problems. In the hypergraph matching setting, we adapt the technique of Guruganesh, Lee (2018) to non-constructively prove that the correlation gap is at least $\\frac{1-e^{-k}}{k}$ and provide a monotone $\\left(b,\\frac{1-e^{-bk}}{bk}\\right)$-balanced contention resolution scheme, generalizing the results of Bruggmann, Zenklusen (2019). For the knapsack problem, we prove that the correlation gap of instances where exactly $k$ copies of each item fit into the knapsack is at least $\\frac{1-e^{-2}}{2}$ and provide several monotone contention resolution schemes: a $\\frac{1-e^{-2}}{2}$-balanced scheme for instances where all item sizes are strictly bigger than $\\frac{1}{2}$, a $\\frac{4}{9}$-balanced scheme for instances where all item sizes are at most $\\frac{1}{2}$, and a $0.279$-balanced scheme for instances with arbitrary item sizes. For $k$-column sparse packing integer programs, we slightly modify the $\\left(2k+o\\left(k\\right)\\right)$-approximation algorithm for $k$-CS-PIP based on the strengthened LP relaxation presented in Brubach et al. (2019) to obtain a $\\frac{1}{4k+o\\left(k\\right)}$-balanced contention resolution scheme and hence a $\\left(4k+o\\left(k\\right)\\right)$-approximation algorithm for $k$-CS-PIP based on the natural LP relaxation."}
{"main_page": "https://arxiv.org/abs/2404.00416", "pdf": "https://arxiv.org/pdf/2404.00416", "title": "Circular-arc graphs and the Helly property", "authors": "Jan Derbisz, Tomasz Krawczyk", "subjects": "Data Structures and Algorithms (cs.DS); Discrete Mathematics (cs.DM)", "abstract": "In this paper we investigate some problems related to the Helly properties of circular-arc graphs, which are defined as intersection graphs of arcs of a fixed circle. As such, circular-arc graphs are among the simplest classes of intersection graphs whose models might not satisfy the Helly property. In particular, some cliques of a circular-arc graph might be Helly in some but not all arc intersection models of the graph. Our first result is an alternative proof of a theorem by Lin and Szwarcfiter which asserts that for every circular-arc graph $G$ either every normalized model of $G$ satisfies the Helly property or no normalized model of $G$ satisfies this property. Further, we study the Helly properties of a single clique of a circular-arc graph $G$. We divide the cliques of $G$ into three types: a clique $C$ of $G$ is always-Helly/always-non-Helly/ambiguous if $C$ is Helly in every/no/(some but not all) normalized model of $G$. We provide a combinatorial description for the cliques of each type, and based on it, we devise a polynomial time algorithm which determines the type of a given clique. Finally, we study the Helly Cliques problem, in which we are given an $n$-vertex circular-arc graph $G$ and some of its cliques $C_1, \\ldots, C_k$ and we ask if there is an arc intersection model of $G$ in which all the cliques $C_1, \\ldots, C_k$ satisfy the Helly property. We show that: (1) the Helly Cliques problem admits a $2^{O(k\\log{k})}n^{O(1)}$-time algorithm (that is, it is FPT when parametrized by the number of cliques given in the input), (2) assuming Exponential Time Hypothesis (ETH), the Helly Cliques problem cannot be solved in time $2^{o(k)}n^{O(1)}$, (3) the Helly Cliques problem admits a polynomial kernel of size $O(k^6)$. All our results use a data structure, called a PQM-tree, which maintains all normalized models of a circular-arc graph $G$."}
{"main_page": "https://arxiv.org/abs/2404.00527", "pdf": "https://arxiv.org/pdf/2404.00527", "title": "Prophet Inequalities with Cancellation Costs", "authors": "Farbod Ekbatani, Rad Niazadeh, Pranav Nuti, Jan Vondrak", "subjects": "Data Structures and Algorithms (cs.DS); Computer Science and Game Theory (cs.GT)", "abstract": "Most of the literature on online algorithms and sequential decision-making focuses on settings with \"irrevocable decisions\" where the algorithm's decision upon arrival of the new input is set in stone and can never change in the future. One canonical example is the classic prophet inequality problem, where realizations of a sequence of independent random variables $X_1, X_2,\\ldots$ with known distributions are drawn one by one and a decision maker decides when to stop and accept the arriving random variable, with the goal of maximizing the expected value of their pick. We consider \"prophet inequalities with recourse\" in the linear buyback cost setting, where after accepting a variable $X_i$, we can still discard $X_i$ later and accept another variable $X_j$, at a \\textit{buyback cost} of $f \\times X_i$. The goal is to maximize the expected net reward, which is the value of the final accepted variable minus the total buyback cost. Our first main result is an optimal prophet inequality in the regime of $f \\geq 1$, where we prove that we can achieve an expected reward $\\frac{1+f}{1+2f}$ times the expected offline optimum. The problem is still open for $0<f<1$ and we give some partial results in this regime. In particular, as our second main result, we characterize the asymptotic behavior of the competitive ratio for small $f$ and provide almost matching upper and lower bounds that show a factor of $1-\\Theta\\left(f\\log(\\frac{1}{f})\\right)$. Our results are obtained by two fundamentally different approaches: One is inspired by various proofs of the classical prophet inequality, while the second is based on combinatorial optimization techniques involving LP duality, flows, and cuts."}
{"main_page": "https://arxiv.org/abs/2404.00529", "pdf": "https://arxiv.org/pdf/2404.00529", "title": "Super Non-singular Decompositions of Polynomials and their Application  to Robustly Learning Low-degree PTFs", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Sihan Liu, Nikos Zarifis", "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)", "abstract": "We study the efficient learnability of low-degree polynomial threshold functions (PTFs) in the presence of a constant fraction of adversarial corruptions. Our main algorithmic result is a polynomial-time PAC learning algorithm for this concept class in the strong contamination model under the Gaussian distribution with error guarantee $O_{d, c}(\\text{opt}^{1-c})$, for any desired constant $c>0$, where $\\text{opt}$ is the fraction of corruptions. In the strong contamination model, an omniscient adversary can arbitrarily corrupt an $\\text{opt}$-fraction of the data points and their labels. This model generalizes the malicious noise model and the adversarial label noise model. Prior to our work, known polynomial-time algorithms in this corruption model (or even in the weaker adversarial label noise model) achieved error $\\tilde{O}_d(\\text{opt}^{1/(d+1)})$, which deteriorates significantly as a function of the degree $d$. Our algorithm employs an iterative approach inspired by localization techniques previously used in the context of learning linear threshold functions. Specifically, we use a robust perceptron algorithm to compute a good partial classifier and then iterate on the unclassified points. In order to achieve this, we need to take a set defined by a number of polynomial inequalities and partition it into several well-behaved subsets. To this end, we develop new polynomial decomposition techniques that may be of independent interest."}
{"main_page": "https://arxiv.org/abs/2404.00683", "pdf": "https://arxiv.org/pdf/2404.00683", "title": "Improved approximation ratio for covering pliable set families", "authors": "Zeev Nutov", "subjects": "Data Structures and Algorithms (cs.DS)", "abstract": "A classic result of Williamson, Goemans, Mihail, and Vazirani [STOC 1993: 708-717] states that the problem of covering an uncrossable set family by a min-cost edge set admits approximation ratio $2$, by a primal-dual algorithm with a reverse delete phase. Recently, Bansal, Cheriyan, Grout, and Ibrahimpur [ICALP 2023: 15:1-15:19] showed that this algorithm achieves approximation ratio $16$ for a larger class of set families, that have much weaker uncrossing properties. In this paper we will refine their analysis and show an approximation ratio of $10$. This also improves approximation ratios for several variants of the Capacitated $k$-Edge Connected Spanning Subgraph problem."}
{"main_page": "https://arxiv.org/abs/2404.00768", "pdf": "https://arxiv.org/pdf/2404.00768", "title": "Adversarially-Robust Inference on Trees via Belief Propagation", "authors": "Samuel B. Hopkins, Anqi Li", "subjects": "Data Structures and Algorithms (cs.DS); Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)", "abstract": "We introduce and study the problem of posterior inference on tree-structured graphical models in the presence of a malicious adversary who can corrupt some observed nodes. In the well-studied broadcasting on trees model, corresponding to the ferromagnetic Ising model on a $d$-regular tree with zero external field, when a natural signal-to-noise ratio exceeds one (the celebrated Kesten-Stigum threshold), the posterior distribution of the root given the leaves is bounded away from $\\mathrm{Ber}(1/2)$, and carries nontrivial information about the sign of the root. This posterior distribution can be computed exactly via dynamic programming, also known as belief propagation. We first confirm a folklore belief that a malicious adversary who can corrupt an inverse-polynomial fraction of the leaves of their choosing makes this inference impossible. Our main result is that accurate posterior inference about the root vertex given the leaves is possible when the adversary is constrained to make corruptions at a $\\rho$-fraction of randomly-chosen leaf vertices, so long as the signal-to-noise ratio exceeds $O(\\log d)$ and $\\rho \\leq c \\varepsilon$ for some universal $c > 0$. Since inference becomes information-theoretically impossible when $\\rho \\gg \\varepsilon$, this amounts to an information-theoretically optimal fraction of corruptions, up to a constant multiplicative factor. Furthermore, we show that the canonical belief propagation algorithm performs this inference."}

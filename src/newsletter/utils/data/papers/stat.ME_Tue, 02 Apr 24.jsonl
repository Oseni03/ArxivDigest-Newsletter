{"main_page": "https://arxiv.org/abs/2404.00221", "pdf": "https://arxiv.org/pdf/2404.00221", "title": "Robust Learning for Optimal Dynamic Treatment Regimes with Observational  Data", "authors": "Shosei Sakaguchi", "subjects": "Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)", "abstract": "Many public policies and medical interventions involve dynamics in their treatment assignments, where treatments are sequentially assigned to the same individuals across multiple stages, and the effect of treatment at each stage is usually heterogeneous with respect to the history of prior treatments and associated characteristics. We study statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's history. We propose a step-wise doubly-robust approach to learn the optimal DTR using observational data under the assumption of sequential ignorability. The approach solves the sequential treatment assignment problem through backward induction, where, at each step, we combine estimators of propensity scores and action-value functions (Q-functions) to construct augmented inverse probability weighting estimators of values of policies for each stage. The approach consistently estimates the optimal DTR if either a propensity score or Q-function for each stage is consistently estimated. Furthermore, the resulting DTR can achieve the optimal convergence rate $n^{-1/2}$ of regret under mild conditions on the convergence rate for estimators of the nuisance parameters."}
{"main_page": "https://arxiv.org/abs/2404.00256", "pdf": "https://arxiv.org/pdf/2404.00256", "title": "Objective Bayesian FDR", "authors": "Yoshiko Hayashi", "subjects": "Methodology (stat.ME)", "abstract": "Here, we develop an objective Bayesian analysis for large-scale datasets. When Bayesian analysis is applied to large-scale datasets, the cut point that provides the posterior probability is usually determined following customs. In this work, we propose setting the cut point in an objective manner, which is determined so as to match the posterior null number with the estimated true null number. The posterior probability obtained using an objective cut point is relatively similar to the real false discovery rate (FDR), which facilitates control of the FDR level."}
{"main_page": "https://arxiv.org/abs/2404.00319", "pdf": "https://arxiv.org/pdf/2404.00319", "title": "Direction Preferring Confidence Intervals", "authors": "Tzviel Frostig, Yoav Benjamini, Ruth Heller", "subjects": "Methodology (stat.ME); Applications (stat.AP)", "abstract": "Confidence intervals (CIs) are instrumental in statistical analysis, providing a range estimate of the parameters. In modern statistics, selective inference is common, where only certain parameters are highlighted. However, this selective approach can bias the inference, leading some to advocate for the use of CIs over p-values. To increase the flexibility of confidence intervals, we introduce direction-preferring CIs, enabling analysts to focus on parameters trending in a particular direction. We present these types of CIs in two settings: First, when there is no selection of parameters; and second, for situations involving parameter selection, where we offer a conditional version of the direction-preferring CIs. Both of these methods build upon the foundations of Modified Pratt CIs, which rely on non-equivariant acceptance regions to achieve longer intervals in exchange for improved sign exclusions. We show that for selected parameters out of m > 1 initial parameters of interest, CIs aimed at controlling the false coverage rate, have higher power to determine the sign compared to conditional CIs. We also show that conditional confidence intervals control the marginal false coverage rate (mFCR) under any dependency."}
{"main_page": "https://arxiv.org/abs/2404.00359", "pdf": "https://arxiv.org/pdf/2404.00359", "title": "Loss-based prior for tree topologies in BART models", "authors": "F. Serafini, F. Leisen, C. Villa, K. Wilson", "subjects": "Methodology (stat.ME); Applications (stat.AP)", "abstract": "We present a novel prior for tree topology within Bayesian Additive Regression Trees (BART) models. This approach quantifies the hypothetical loss in information and the loss due to complexity associated with choosing the wrong tree structure. The resulting prior distribution is compellingly geared toward sparsity, a critical feature considering BART models' tendency to overfit. Our method incorporates prior knowledge into the distribution via two parameters that govern the tree's depth and balance between its left and right branches. Additionally, we propose a default calibration for these parameters, offering an objective version of the prior. We demonstrate our method's efficacy on both simulated and real datasets."}
{"main_page": "https://arxiv.org/abs/2404.00606", "pdf": "https://arxiv.org/pdf/2404.00606", "title": "\"Sound and Fury\": Nonlinear Functionals of Volatility Matrix in the  Presence of Jump and Noise", "authors": "Richard Y. Chen", "subjects": "Methodology (stat.ME)", "abstract": "This paper resolves a pivotal open problem on nonparametric inference for nonlinear functionals of volatility matrix. Multiple prominent statistical tasks can be formulated as functionals of volatility matrix, yet a unified statistical theory of general nonlinear functionals based on noisy data remains challenging and elusive. Nonetheless, this paper shows it can be achieved by combining the strengths of pre-averaging, jump truncation and nonlinearity bias correction. In light of general nonlinearity, bias correction beyond linear approximation becomes necessary. Resultant estimators are nonparametric and robust over a wide spectrum of stochastic models. Moreover, the estimators can be rate-optimal and stable central limit theorems are obtained. The proposed framework lends itself conveniently to uncertainty quantification and permits fully feasible inference. With strong theoretical guarantees, this paper provides an inferential foundation for a wealth of statistical methods for noisy high-frequency data, such as realized principal component analysis, continuous-time linear regression, realized Laplace transform, generalized method of integrated moments and specification tests, hence extends current application scopes to noisy data which is more prevalent in practice."}
{"main_page": "https://arxiv.org/abs/2404.00735", "pdf": "https://arxiv.org/pdf/2404.00735", "title": "Two-Stage Nuisance Function Estimation for Causal Mediation Analysis", "authors": "AmirEmad Ghassami", "subjects": "Methodology (stat.ME); Machine Learning (stat.ML)", "abstract": "When estimating the direct and indirect causal effects using the influence function-based estimator of the mediation functional, it is crucial to understand what aspects of the treatment, the mediator, and the outcome mean mechanisms should be focused on. Specifically, considering them as nuisance functions and attempting to fit these nuisance functions as accurate as possible is not necessarily the best approach to take. In this work, we propose a two-stage estimation strategy for the nuisance functions that estimates the nuisance functions based on the role they play in the structure of the bias of the influence function-based estimator of the mediation functional. We provide robustness analysis of the proposed method, as well as sufficient conditions for consistency and asymptotic normality of the estimator of the parameter of interest."}
{"main_page": "https://arxiv.org/abs/2404.00788", "pdf": "https://arxiv.org/pdf/2404.00788", "title": "A Novel Stratified Analysis Method for Testing and Estimating Overall  Treatment Effects on Time-to-Event Outcomes Using Average Hazard with  Survival Weight", "authors": "Zihan Qian, Lu Tian, Miki Horiguchi, Hajime Uno", "subjects": "Methodology (stat.ME)", "abstract": "Given the limitations of using the Cox hazard ratio to summarize the magnitude of the treatment effect, alternative measures that do not have these limitations are gaining attention. One of the recently proposed alternative methods uses the average hazard with survival weight (AH). This population quantity can be interpreted as the average intensity of the event occurrence in a given time window that does not involve study-specific censoring. Inference procedures for the ratio of AH and difference in AH have already been proposed in simple randomized controlled trial settings to compare two groups. However, methods with stratification factors have not been well discussed, although stratified analysis is often used in practice to adjust for confounding factors and increase the power to detect a between-group difference. The conventional stratified analysis or meta-analysis approach, which integrates stratum-specific treatment effects using an optimal weight, directly applies to the ratio of AH and difference in AH. However, this conventional approach has significant limitations similar to the Cochran-Mantel-Haenszel method for a binary outcome and the stratified Cox procedure for a time-to-event outcome. To address this, we propose a new stratified analysis method for AH using standardization. With the proposed method, one can summarize the between-group treatment effect in both absolute difference and relative terms, adjusting for stratification factors. This can be a valuable alternative to the traditional stratified Cox procedure to estimate and report the magnitude of the treatment effect on time-to-event outcomes using hazard."}
{"main_page": "https://arxiv.org/abs/2404.00820", "pdf": "https://arxiv.org/pdf/2404.00820", "title": "Visual analysis of bivariate dependence between continuous random  variables", "authors": "Arturo Erdely, Manuel Rubio-Sanchez", "subjects": "Methodology (stat.ME)", "abstract": "Scatter plots are widely recognized as fundamental tools for illustrating the relationship between two numerical variables. Despite this, based on solid theoretical foundations, scatter plots generated from pairs of continuous random variables may not serve as reliable tools for assessing dependence. Sklar's Theorem implies that scatter plots created from ranked data are preferable for such analysis as they exclusively convey information pertinent to dependence. This is in stark contrast to conventional scatter plots, which also encapsulate information about the variables' marginal distributions. Such additional information is extraneous to dependence analysis and can obscure the visual interpretation of the variables' relationship. In this article, we delve into the theoretical underpinnings of these ranked data scatter plots, hereafter referred to as rank plots. We offer insights into interpreting the information they reveal and examine their connections with various association measures, including Pearson's and Spearman's correlation coefficients, as well as Schweizer-Wolff's measure of dependence. Furthermore, we introduce a novel graphical combination for dependence analysis, termed a dplot, and demonstrate its efficacy through real data examples."}
{"main_page": "https://arxiv.org/abs/2404.01043", "pdf": "https://arxiv.org/pdf/2404.01043", "title": "The Mean Shape under the Relative Curvature Condition", "authors": "Mohsen Taheri, Stephen M. Pizer, J\u00f6rn Schulz", "subjects": "Methodology (stat.ME)", "abstract": "The relative curvature condition (RCC) serves as a crucial constraint, ensuring the avoidance of self-intersection problems in calculating the mean shape over a sample of swept regions. By considering the RCC, this work discusses estimating the mean shape for a class of swept regions called elliptical slabular objects based on a novel shape representation, namely elliptical tube representation (ETRep). The ETRep shape space equipped with extrinsic and intrinsic distances in accordance with object transformation is explained. The intrinsic distance is determined based on the intrinsic skeletal coordinate system of the shape space. Further, calculating the intrinsic mean shape based on the intrinsic distance over a set of ETReps is demonstrated. The proposed intrinsic methodology is applied for the statistical shape analysis to design global and partial hypothesis testing methods to study the hippocampal structure in early Parkinson's disease."}
{"main_page": "https://arxiv.org/abs/2404.01076", "pdf": "https://arxiv.org/pdf/2404.01076", "title": "Debiased calibration estimation using generalized entropy in survey  sampling", "authors": "Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu", "subjects": "Methodology (stat.ME)", "abstract": "Incorporating the auxiliary information into the survey estimation is a fundamental problem in survey sampling. Calibration weighting is a popular tool for incorporating the auxiliary information. The calibration weighting method of Deville and Sarndal (1992) uses a distance measure between the design weights and the final weights to solve the optimization problem with calibration constraints. This paper introduces a novel framework that leverages generalized entropy as the objective function for optimization, where design weights play a role in the constraints to ensure design consistency, rather than being part of the objective function. This innovative calibration framework is particularly attractive due to its generality and its ability to generate more efficient calibration weights compared to traditional methods based on Deville and Sarndal (1992). Furthermore, we identify the optimal choice of the generalized entropy function that achieves the minimum variance across various choices of the generalized entropy function under the same constraints. Asymptotic properties, such as design consistency and asymptotic normality, are presented rigorously. The results from a limited simulation study are also presented. We demonstrate a real-life application using agricultural survey data collected from Kynetec, Inc."}
{"main_page": "https://arxiv.org/abs/2404.01191", "pdf": "https://arxiv.org/pdf/2404.01191", "title": "A Semiparametric Approach for Robust and Efficient Learning with Biobank  Data", "authors": "Molei Liu, Xinyi Wang, Chuan Hong", "subjects": "Methodology (stat.ME)", "abstract": "With the increasing availability of electronic health records (EHR) linked with biobank data for translational research, a critical step in realizing its potential is to accurately classify phenotypes for patients. Existing approaches to achieve this goal are based on error-prone EHR surrogate outcomes, assisted and validated by a small set of labels obtained via medical chart review, which may also be subject to misclassification. Ignoring the noise in these outcomes can induce severe estimation and validation bias to both EHR phenotyping and risking modeling with biomarkers collected in the biobank. To overcome this challenge, we propose a novel unsupervised and semiparametric approach to jointly model multiple noisy EHR outcomes with their linked biobank features. Our approach primarily aims at disease risk modeling with the baseline biomarkers, and is also able to produce a predictive EHR phenotyping model and validate its performance without observations of the true disease outcome. It consists of composite and nonparametric regression steps free of any parametric model specification, followed by a parametric projection step to reduce the uncertainty and improve the estimation efficiency. We show that our method is robust to violations of the parametric assumptions while attaining the desirable root-$n$ convergence rates on risk modeling. Our developed method outperforms existing methods in extensive simulation studies, as well as a real-world application in phenotyping and genetic risk modeling of type II diabetes."}

{"main_page": "https://arxiv.org/abs/2404.00002", "pdf": "https://arxiv.org/pdf/2404.00002", "title": "The Minimum $L_2$-Distance Projection onto the Canonical Simplex: A  Simple Algorithm", "authors": "Hans J. H. Tuenter", "subjects": "Optimization and Control (math.OC)", "abstract": "We consider the minimum distance projection in the $L_2$-norm from an arbitrary point in an $n$-dimensional, Euclidian space onto the canonical simplex. It is shown that this problem reduces to a univariate problem that can be solved by a simple algorithm. This optimization problem occurs in the setting of credit risk, where one has stochastic matrices that describe transition probabilities between different credit ratings, and one wants to determine the roots of these matrices, or close approximations to them."}
{"main_page": "https://arxiv.org/abs/2404.00003", "pdf": "https://arxiv.org/pdf/2404.00003", "title": "Algorithms for constrained optimal transport", "authors": "Martin Corless, Anthony Quinn, Sarah Boufelja, Robert Shorten", "subjects": "Optimization and Control (math.OC); Dynamical Systems (math.DS)", "abstract": "We derive iterative scaling algorithms of the Sinkhorn-Knopp (SK) type for constrained optimal transport. The constraints are in the form of prior-imposed zeroes in the transport plan. Based on classical Bregman arguments, we prove asymptotic convergence of our algorithms to a unique optimal solution. New insights obtained from the convergence proof are highlighted. An example from electrical vehicle charging in a smart city context is outlined, in which the prior zero-constraints prevent energy from being transported from some providers to some vehicles."}
{"main_page": "https://arxiv.org/abs/2404.00005", "pdf": "https://arxiv.org/pdf/2404.00005", "title": "Introduction: Swarm-based gradient descent for non convex optimization", "authors": "Janina Tikko", "subjects": "Optimization and Control (math.OC)", "abstract": "The field of optimization has the goal to find an optimal solution to a target function, i.e. to minimize (or maximize) the target function. When trying to find such a global minimum, one often encounters local minima due to unfavorable procedures and starting regions. The swarm-based gradient descent method of Prof. Eitan Tadmor offers an alternative method for solving global minimization problems. By using a swarm of agents, local minima will not be taken account and the global minimum will be found. Furthermore leads the communication between the agent to a further expansion of the search region. Under the supervision of Prof. Angela Kunoth, I give an introduction to this swarm-based method in my bachelor thesis. Therefore I used my own program in Julia to give a more visual understanding of how the new method works and which influence certain parameters such as the number of agents or \"q\" have."}
{"main_page": "https://arxiv.org/abs/2404.00008", "pdf": "https://arxiv.org/pdf/2404.00008", "title": "Best free knot linear spline approximation and its application to neural  networks", "authors": "Vinesha Peiris, Duy Khoa Pham, Nadezda Sukhorukova", "subjects": "Optimization and Control (math.OC)", "abstract": "The problem of fixed knot approximation is convex and there are several efficient approaches to solve this problem, yet, when the knots joining the affine parts are also variable, finding conditions for a best Chebyshev approximation remains an open problem. It was noticed before that piecewise linear approximation with free knots is equivalent to neural network approximation with piecewise linear activation functions (for example ReLU). In this paper, we demonstrate that in the case of one internal free knot, the problem of linear spline approximation can be reformulated as a mixed-integer linear programming problem and solved efficiently using, for example, a branch and bound type method. We also present a new sufficient optimality condition for a one free knot piecewise linear approximation. The results of numerical experiments are provided."}
{"main_page": "https://arxiv.org/abs/2404.00010", "pdf": "https://arxiv.org/pdf/2404.00010", "title": "Technical Report: Pose Graph Optimization over Planar Unit Dual  Quaternions: Improved Accuracy with Provably Convergent Riemannian  Optimization", "authors": "William D. Warke, J. Humberto Ramos, Prashant Ganesh, Kevin M. Brink, Matthew T. Hale", "subjects": "Optimization and Control (math.OC)", "abstract": "It is common in pose graph optimization (PGO) algorithms to assume that noise in the translations and rotations of relative pose measurements is uncorrelated. However, existing work shows that in practice these measurements can be highly correlated, which leads to degradation in the accuracy of PGO solutions that rely on this assumption. Therefore, in this paper we develop a novel algorithm derived from a realistic, correlated model of relative pose uncertainty, and we quantify the resulting improvement in the accuracy of the solutions we obtain relative to state-of-the-art PGO algorithms. Our approach utilizes Riemannian optimization on the planar unit dual quaternion (PUDQ) manifold, and we prove that it converges to first-order stationary points of a Lie-theoretic maximum likelihood objective. Then we show experimentally that, compared to state-of-the-art PGO algorithms, this algorithm produces estimation errors that are lower by 10% to 25% across several orders of magnitude of noise levels and graph sizes."}
{"main_page": "https://arxiv.org/abs/2404.00038", "pdf": "https://arxiv.org/pdf/2404.00038", "title": "Strong convergence towards the minimum norm solution via temporal  scaling and Tikhonov approximation of a first-order dynamical system", "authors": "A. C. Bagy, Z. Chbani, H. Riahi", "subjects": "Optimization and Control (math.OC); Dynamical Systems (math.DS)", "abstract": "Given a proper convex lower semicontinuous function defined on a Hilbert space and whose solution set is supposed nonempty. For attaining a global minimizer when this convex function is continuously differentiable, we approach it by a first-order continuous dynamical system with a time rescaling parameter and a Tikhonov regularization term. We show, along the generated trajectories, fast convergence of values, fast convergence of gradients towards origin and strong convergence towards the minimum norm element of the solution set. These convergence rates now depend on the time rescaling parameter, and thus improve existing results by choosing this parameter appropriately. The obtained results illustrate, via particular cases on the choice of the time rescaling parameter, good performances of the proposed continuous method and the wide range of applications they can address. Numerical illustrations for continuous example are provided to confirm the theoretical results."}
{"main_page": "https://arxiv.org/abs/2404.00042", "pdf": "https://arxiv.org/pdf/2404.00042", "title": "Stochastic Optimization with Constraints: A Non-asymptotic  Instance-Dependent Analysis", "authors": "Koulik Khamaru", "subjects": "Optimization and Control (math.OC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \\rightarrow \\infty$, the VRPG algorithm achieves the renowned local minimax lower bound by H\\`{a}jek and Le Cam up to universal constants and a logarithmic factor of the sample size."}
{"main_page": "https://arxiv.org/abs/2404.00046", "pdf": "https://arxiv.org/pdf/2404.00046", "title": "Partial Backorder Inventory System: Asymptotic Optimality and Demand  Learning", "authors": "Andrew E. B. Lim, Zhao-Xuan Wei, Hanqin Zhang", "subjects": "Optimization and Control (math.OC); Probability (math.PR)", "abstract": "We develop a stochastic inventory system which accounts for the limited patience of backlogged customers. While limited patience is a feature that is closer to the nature of unmet demand, our model also unifies the classic backlogging and lost-sales inventory systems which are special cases of the one we propose. We establish the uniform (asymptotic) optimality of the base-stock policy when both demand and patience distributions are known. When the backlogged demands become unobservable, we introduce a novel policy family that operates without backlogged demands information, and prove that it can approach the cost efficiency of the optimal policy in the system when the demand and patience distributions are known. Finally, we consider an online inventory control problem in which backlogged demand is unobservable and demand and patience distributions are also not known, and develop a UCB-type algorithm that yields a near-optimal policy. The regret bounds given by the algorithm are provably tight within the planning horizon, and are comparable to the state-of-the-art results in the literature, even in the face of partial and biased observations and weaker system ergodicity."}
{"main_page": "https://arxiv.org/abs/2404.00053", "pdf": "https://arxiv.org/pdf/2404.00053", "title": "Adaptive Computing for Scale-up Problems", "authors": "Hilary Egan, Kevin Patrick Griffin, Marc T. Henry de Frahan, Juliane Mueller, Deepthi Vaidhynatha, Dylan Wald, Rohit Chintala, Olga A. Doronina, Ryan King, Jibonananda Sanyal, Marc Day", "subjects": "Optimization and Control (math.OC); Fluid Dynamics (physics.flu-dyn)", "abstract": "Adaptive Computing is an application-agnostic outer loop framework to strategically deploy simulations and experiments to guide decision making for scale-up analysis. Resources are allocated over successive batches, which makes the allocation adaptive to some objective such as optimization or model training. The framework enables the characterization and management of uncertainties associated with predictive models of complex systems when scale-up questions lead to significant model extrapolation. A key feature of this framework is the ability to explicitly utilize user-specified uncertainty priors, which we call model-specific local trust estimates, that are provided directly together with the problem specification and exploited in adaptive sampling strategies. A multi-fidelity model hierarchy is supported to allow trade-offs in accuracy and data acquisition cost while exploring the search space given a specified budget of potentially distributed, heterogeneous resources. We discuss application of this framework to problems in the renewable energy space, including biofuels production, material synthesis, perovskite crystal growth, and building electrical loads."}
{"main_page": "https://arxiv.org/abs/2404.00055", "pdf": "https://arxiv.org/pdf/2404.00055", "title": "Efficient Global Algorithms for Transmit Beamforming Design in ISAC  Systems", "authors": "Jiageng Wu, Zhiguo Wang, Ya-Feng Liu, Fan Liu", "subjects": "Optimization and Control (math.OC)", "abstract": "In this paper, we propose a multi-input multi-output transmit beamforming optimization model for joint radar sensing and multi-user communications, where the design of the beamformers is formulated as an optimization problem whose objective is a weighted combination of the sum rate and the Cram\\'{e}r-Rao bound, subject to the transmit power budget. Obtaining the global solution for the formulated nonconvex problem is a challenging task, since the sum-rate maximization problem itself (even without considering the sensing metric) is known to be NP-hard. The main contributions of this paper are threefold. Firstly, we derive an optimal closed-form solution to the formulated problem in the single-user case and the multi-user case where the channel vectors of different users are orthogonal. Secondly, for the general multi-user case, we propose a novel branch and bound (B\\&B) algorithm based on the McCormick envelope relaxation. The proposed algorithm is guaranteed to find the globally optimal solution to the formulated problem. Thirdly, we design a graph neural network (GNN) based pruning policy to determine irrelevant nodes that can be directly pruned in the proposed B\\&B algorithm, thereby significantly reducing the number of unnecessary enumerations therein and improving its computational efficiency. Simulation results show the efficiency of the proposed vanilla and GNN-based accelerated B\\&B algorithms."}
{"main_page": "https://arxiv.org/abs/2404.00112", "pdf": "https://arxiv.org/pdf/2404.00112", "title": "An SVD-like Decomposition of Bounded-Input Bounded-Output Functions", "authors": "Brian Charles Brown, Michael King, Sean Warnick, Enoch Yeung, David Grimsman", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "The Singular Value Decomposition (SVD) of linear functions facilitates the calculation of their 2-induced norm and row and null spaces, hallmarks of linear control theory. In this work, we present a function representation that, similar to SVD, provides an upper bound on the 2-induced norm of bounded-input bounded-output functions, as well as facilitates the computation of generalizations of the notions of row and null spaces. Borrowing from the notion of \"lifting\" in Koopman operator theory, we construct a finite-dimensional lifting of inputs that relaxes the unitary property of the right-most matrix in traditional SVD, $V^*$, to be an injective, norm-preserving mapping to a slightly higher-dimensional space."}
{"main_page": "https://arxiv.org/abs/2404.00158", "pdf": "https://arxiv.org/pdf/2404.00158", "title": "Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing", "authors": "Alireza Aghasi, Saeed Ghadimi", "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)", "abstract": "In this paper, we study and analyze zeroth-order stochastic approximation algorithms for solving bilvel problems, when neither the upper/lower objective values, nor their unbiased gradient estimates are available. In particular, exploiting Stein's identity, we first use Gaussian smoothing to estimate first- and second-order partial derivatives of functions with two independent block of variables. We then used these estimates in the framework of a stochastic approximation algorithm for solving bilevel optimization problems and establish its non-asymptotic convergence analysis. To the best of our knowledge, this is the first time that sample complexity bounds are established for a fully stochastic zeroth-order bilevel optimization algorithm."}
{"main_page": "https://arxiv.org/abs/2404.00170", "pdf": "https://arxiv.org/pdf/2404.00170", "title": "Dynamic Pedestrian Traffic Assignment with Link Transmission Model for  Bidirectional Sidewalk Networks", "authors": "Tanapon Lilasathapornkit, Meead Saberi", "subjects": "Optimization and Control (math.OC); Statistical Mechanics (cond-mat.stat-mech)", "abstract": "Planning assessment of the urban walking infrastructure requires appropriate methodologies that can capture the time-dependent and unique microscopic characteristics of bidirectional pedestrian flow. In this paper, we develop a simulation-based dynamic pedestrian traffic assignment (DPTA) model specifically formulated for walking networks (e.g. sidewalks) with bidirectional links. The model consists of a dynamic user equilibrium (DUE) based route choice and a link transmission model (LTM) for network loading. The formulated DUE adopts a pedestrian volume delay function (pVDF) taking into account the properties of bidirectional pedestrian streams such as self-organization. The adopted LTM uses a three-dimensional triangular bidirectional fundamental diagram as well as a generalized first-order node model. The applicability and validity of the model is demonstrated in hypothetical small networks as well as a real-world large-scale network of sidewalks in Sydney. The model successfully replicates formation and propagation of shockwaves in walking corridors and networks due to bidirectional effects."}
{"main_page": "https://arxiv.org/abs/2404.00178", "pdf": "https://arxiv.org/pdf/2404.00178", "title": "Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues", "authors": "Ali Hassanzadeh, Mojtaba Hosseini, John G. Turner", "subjects": "Optimization and Control (math.OC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive model. We introduce a deterministic equivalent reformulation along with a tailored Frank-Wolfe algorithm to efficiently solve our problem, as well as a robust counterpart based on min-max regret. Results: We present simulation-based numerical experiments from previous National Basketball Association (NBA) seasons 2004--2019, and show that our models are computationally efficient, outperform a greedy benchmark that approximates a non-rankings-based scheduling policy, and produce interpretable results. Managerial implications: Our data-driven decision-making framework may be used to produce a shortened season with 25-50\\% fewer games while still producing an end-of-season ranking similar to that of the full season, had it been played."}
{"main_page": "https://arxiv.org/abs/2404.00199", "pdf": "https://arxiv.org/pdf/2404.00199", "title": "An Efficient Sparse Identification Algorithm For Stochastic Systems With  General Observation Sequences", "authors": "Ziming Wang, Xinghua Zhu", "subjects": "Optimization and Control (math.OC); Dynamical Systems (math.DS)", "abstract": "This paper studies the sparse identification problem of unknown sparse parameter vectors in stochastic dynamic systems. Firstly, a novel sparse identification algorithm is proposed, which can generate sparse estimates based on least squares estimation by adaptively adjusting the threshold. Secondly, under a possibly weakest non-persistent excited condition, we prove that the proposed algorithm can correctly identify the zero and nonzero elements of the sparse parameter vector using a finite number of observations, and further estimates of the nonzero elements almost surely converge to the true values. Compared with the related works, e.g., LASSO, our method only requires the weakest assumptions and does not require solving additional optimization problems. Besides, our theoretical results do not require any statistical assumptions on the regression signals, including independence or stationarity, which makes our results promising for application to stochastic feedback systems. Thirdly, the number of finite observations that guarantee the convergence of the zero-element set of unknown sparse parameters of the Hammerstein system is derived for the first time. Finally, numerical simulations are provided, demonstrating the effectiveness of the proposed method. Since there is no additional optimization problem, i.e., no additional numerical error, the proposed algorithm performs much better than other related algorithms."}
{"main_page": "https://arxiv.org/abs/2404.00382", "pdf": "https://arxiv.org/pdf/2404.00382", "title": "Non-homogeneous stochastic linear-quadratic optimal control problems  with multi-dimensional state and regime switching", "authors": "Yuyang Chen, Peng Luo", "subjects": "Optimization and Control (math.OC)", "abstract": "In this paper, we study non-homogeneous stochastic linear-quadratic (LQ) optimal control problems with multi-dimensional state and regime switching. We focus on the corresponding stochastic Riccati equation, which is the same as that one in homogeneous stochastic LQ optimal control problem, and the adjoint backward stochastic differential equation (BSDE), which arises from the non-homogeneous terms in the state equation and cost functional. Both stochastic Riccati equation and adjoint BSDE are solved by the contraction mapping method, and are used to represent the closed-loop optimal control and the optimal value of our problems."}
{"main_page": "https://arxiv.org/abs/2404.00390", "pdf": "https://arxiv.org/pdf/2404.00390", "title": "Learning truly monotone operators with applications to nonlinear inverse  problems", "authors": "Younes Belkouchi, Jean-Christophe Pesquet, Audrey Repetti, Hugues Talbot", "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Numerical Analysis (math.NA)", "abstract": "This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF algorithm, we then show simulation examples where the non-linear inverse problem is successfully solved."}
{"main_page": "https://arxiv.org/abs/2404.00543", "pdf": "https://arxiv.org/pdf/2404.00543", "title": "Dynamic Transfer Policies for Parallel Queues", "authors": "Timothy C. Y. Chan, Jangwon Park, Vahid Sarhangian", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "We consider the problem of load balancing in parallel queues by transferring customers between them at discrete points in time. Holding costs accrue as customers wait in the queue, while transfer decisions incur both fixed (setup) and variable costs proportional to the number and direction of transfers. Our work is primarily motivated by inter-facility patient transfers between hospitals during a surge in demand for hospitalization (e.g., during a pandemic). By analyzing an associated fluid control problem, we show that under fairly general assumptions including time-varying arrivals and convex increasing holding costs, the optimal policy in each period partitions the state-space into a well-defined $\\textit{no-transfer region}$ and its complement, such that transferring is optimal if and only if the system is sufficiently imbalanced. In the absence of fixed transfer costs, an optimal policy moves the state to the no-transfer region's boundary; in contrast, with fixed costs, the state is moved to the no-transfer region's relative interior. We further leverage the fluid control problem to provide insights on the trade-off between holding and transfer costs, emphasizing the importance of preventing excessive idleness when transfers are not feasible in continuous-time. Using simulation experiments, we investigate the performance and robustness of the fluid policy for the stochastic system. In particular, our case study calibrated using data during the pandemic in the Greater Toronto Area demonstrates that transferring patients between hospitals could result in up to 27.7% reduction in total cost with relatively few transfers."}
{"main_page": "https://arxiv.org/abs/2404.00568", "pdf": "https://arxiv.org/pdf/2404.00568", "title": "Stochastic-Robust Planning of Networked Hydrogen-Electrical Microgrids:  A Study on Induced Refueling Demand", "authors": "Xunhang Sun, Xiaoyu Cao, Bo Zeng, Qiaozhu Zhai, Tamer Ba\u015far, Xiaohong Guan", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "Hydrogen-electrical (HE) microgrids are increasingly assuming an important role on the pathway toward decarbonization of energy and transportation systems. This paper studies networked HE microgrids planning (NHEMP), considering a critical but often-overlooked issue, i.e., the demand-inducing effect (DIE) associated with infrastructure development decisions. Specifically, higher refueling capacities will attract more refueling demand of hydrogen-powered vehicles (HVs). To capture such interactions between investment decisions and induced refueling demand, we introduce a decision-dependent uncertainty (DDU) set and build a trilevel stochastic-robust formulation. The upper-level determines optimal investment strategies for HE microgrids, the lower-level optimizes the risk-aware operation schedules across a series of stochastic scenarios, and, for each scenario, the middle-level identifies the \"worst\" situation of refueling demand within an individual DDU set to ensure economic feasibility. Then, an adaptive and exact decomposition algorithm, based on Parametric Column-and-Constraint Generation (PC&CG), is customized and developed to address the computational challenge and to quantitatively analyze the impact of DIE. Case studies on an IEEE exemplary system validate the effectiveness of the proposed NHEMP model and the PC&CG algorithm. It is worth highlighting that DIE can make an important contribution to the economic benefits of NHEMP, yet its significance will gradually decrease when the main bottleneck transits to other system restrictions."}
{"main_page": "https://arxiv.org/abs/2404.00605", "pdf": "https://arxiv.org/pdf/2404.00605", "title": "Sparse Extended Mean-Variance-CVaR Portfolios with Short-selling", "authors": "Ahmad Mousavi, Maziar Salahi, Zois Boukouvalas", "subjects": "Optimization and Control (math.OC)", "abstract": "This paper introduces a novel penalty decomposition algorithm customized for addressing the non-differentiable and nonconvex problem of extended mean-variance-CVaR portfolio optimization with short-selling and cardinality constraints. The proposed algorithm solves a sequence of penalty subproblems using a block coordinate descent (BCD) method while striving to fully exploit each component of the objective function or constraints. Through rigorous analysis, the well-posed nature of each subproblem of the BCD method is established, and closed-form solutions are derived where possible. A comprehensive theoretical convergence analysis is provided to confirm the efficacy of the introduced algorithm in reaching a local minimizer of this intractable optimization problem in finance, whereas generic optimization techniques either only capture a partial minimum or are not efficient. Numerical experiments conducted on real-world datasets validate the practical applicability, effectiveness, and robustness of the introduced algorithm across various criteria. Notably, the existence of closed-form solutions within the BCD subproblems prominently underscores the efficiency of our algorithm when compared to state-of-the-art methods."}
{"main_page": "https://arxiv.org/abs/2404.00608", "pdf": "https://arxiv.org/pdf/2404.00608", "title": "Sample Complexity of Chance Constrained Optimization in Dynamic  Environment", "authors": "Apurv Shukla, Qian Zhang, Le Xie", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "We study the scenario approach for solving chance-constrained optimization in time-coupled dynamic environments. Scenario generation methods approximate the true feasible region from scenarios generated independently and identically from the actual distribution. In this paper, we consider this problem in a dynamic environment, where the scenarios are assumed to be drawn sequentially from an unknown and time-varying distribution. Such dynamic environments are driven by changing environmental conditions that could be found in many real-world applications such as energy systems. We couple the time-varying distributions using the Wasserstein metric between the sequence of scenario-generating distributions and the actual chance-constrained distribution. Our main results are bounds on the number of samples essential for ensuring the ex-post risk in chance-constrained optimization problems when the underlying feasible set is convex or non-convex. Finally, our results are illustrated on multiple numerical experiments for both types of feasible sets."}
{"main_page": "https://arxiv.org/abs/2404.00635", "pdf": "https://arxiv.org/pdf/2404.00635", "title": "Popov Mirror-Prox for solving Variational Inequalities", "authors": "Abhishek Chakraborty, Angelia Nedi\u0107", "subjects": "Optimization and Control (math.OC)", "abstract": "We consider the mirror-prox algorithm for solving monotone Variational Inequality (VI) problems. As the mirror-prox algorithm is not practically implementable, except in special instances of VIs (such as affine VIs), we consider its implementation with Popov method updates. We provide convergence rate analysis of our proposed method for a monotone VI with a Lipschitz continuous mapping. We establish a convergence rate of $O(1/t)$, in terms of the number $t$ of iterations, for the dual gap function. Simulations on a two player matrix game corroborate our findings."}
{"main_page": "https://arxiv.org/abs/2404.00697", "pdf": "https://arxiv.org/pdf/2404.00697", "title": "A Lane Usage Strategy for General Traffic Access on Bus Lanes under  Mixed Traffic Environment", "authors": "Haoran Li, Zhenzhou Yuan, Rui Yue, Guangchuan Yang, Chuang Zhu, Siyuan Chen", "subjects": "Optimization and Control (math.OC)", "abstract": "The strategy of permitting general traffic to use the bus lane for improved utilization while ensuring bus priority has gained increasingly attention, particularly with the support of vehicle-to-everything technology. In this study, we propose a novel lane usage strategy called Dynamic Spatial-Temporal Priority (DSTP) to ensure bus priority and optimize bus lane usage in a mixed traffic environment. DSTP leverages dynamic methods to identify available spatial-temporal resources in the lane, utilizing signal timing, road information, and vehicle data. A Right-of-Way assignment optimization model is then developed based on these resources to determine which vehicles can enter the bus lane. The model is dynamically enacted using a rolling horizon scheme to accommodate time-varying traffic conditions. Numerical studies have validated the advantages of DSTP, showing maintained bus priority, improved traffic efficiency, reduced fuel consumption, and lower CO2 emissions, especially during periods of high traffic demand and concentrated bus arrivals."}
{"main_page": "https://arxiv.org/abs/2404.00713", "pdf": "https://arxiv.org/pdf/2404.00713", "title": "Computing Proximity Operators of Scale and Signed Permutation Invariant  Functions", "authors": "Jianqing Jia, Ashley Prater-Bennette, Lixin Shen", "subjects": "Optimization and Control (math.OC)", "abstract": "This paper investigates the computation of proximity operators for scale and signed permutation invariant functions. A scale-invariant function remains unchanged under uniform scaling, while a signed permutation invariant function retains its structure despite permutations and sign changes applied to its input variables. Noteworthy examples include the $\\ell_0$ function and the ratios of $\\ell_1/\\ell_2$ and its square, with their proximity operators being particularly crucial in sparse signal recovery. We delve into the properties of scale and signed permutation invariant functions, delineating the computation of their proximity operators into three sequential steps: the $\\mathbf{w}$-step, $r$-step, and $d$-step. These steps collectively form a procedure termed as WRD, with the $\\mathbf{w}$-step being of utmost importance and requiring careful treatment. Leveraging this procedure, we present a method for explicitly computing the proximity operator of $(\\ell_1/\\ell_2)^2$ and introduce an efficient algorithm for the proximity operator of $\\ell_1/\\ell_2$."}
{"main_page": "https://arxiv.org/abs/2404.00764", "pdf": "https://arxiv.org/pdf/2404.00764", "title": "Sparse Recovery: The Square of $\\ell_1/\\ell_2$ Norms", "authors": "Jianqing Jia, Ashley Prater-Bennette, Lixin Shen, Erin E. Tripp", "subjects": "Optimization and Control (math.OC)", "abstract": "This paper introduces a nonconvex approach to the problem of recovering sparse signals. We propose a novel model, termed the $\\tau_2$-model, which utilizes the square of $\\ell_1/\\ell_2$ norms for sparse recovery. This model is an advancement over the $\\ell_0$ norm, which is often computationally intractable and less effective in handling practical scenarios. Our approach is grounded in the concept of effective sparsity, which robustly measures the number of effective coordinates in a signal. We demonstrate that our model is a powerful alternative for sparse signal estimation, with the $\\tau_2$-model offering computational advantages and practical applicability. The model's formulation and the accompanying algorithm, based on Dinkelbach's procedure combined with a difference of convex functions strategy, are detailed. We further explore the properties of our model, including the existence of solutions under certain conditions, and discuss the algorithm's convergence properties. Numerical experiments with various sensing matrices are conducted to validate the effectiveness of our model."}
{"main_page": "https://arxiv.org/abs/2404.00765", "pdf": "https://arxiv.org/pdf/2404.00765", "title": "Convex Network Flows", "authors": "Theo Diamandis, Guillermo Angeris, Alan Edelman", "subjects": "Optimization and Control (math.OC)", "abstract": "We introduce a general framework for flow problems over hypergraphs. In our problem formulation, which we call the convex flow problem, we have a concave utility function for the net flow at every node and a concave utility function for each edge flow. The objective is to minimize the sum of these utilities, subject to constraints on the flows allowed at each edge, which we only assume to be a convex set. This framework not only includes many classic problems in network optimization, such as max flow, min-cost flow, and multi-commodity flows, but also generalizes these problems to allow, for example, concave edge gain functions. In addition, our framework includes applications spanning a number of fields: optimal power flow over lossy networks, routing and resource allocation in ad-hoc wireless networks, Arrow-Debreu Nash bargaining, and order routing through financial exchanges, among others. We show that the convex flow problem has a dual with a number of interesting interpretations, and that this dual decomposes over the edges of the hypergraph. Using this decomposition, we propose a fast solution algorithm that parallelizes over the edges and admits a clean problem interface. We provide an open source implementation of this algorithm in the Julia programming language, which we show is significantly faster than the state-of-the-art commercial convex solver Mosek."}
{"main_page": "https://arxiv.org/abs/2404.00805", "pdf": "https://arxiv.org/pdf/2404.00805", "title": "An OpenStreetMaps based tool to study the energy demand and emissions  impact of electrification of medium and heavy-duty freight trucks", "authors": "Nawaf Nazir, Bowen Huang, Shant Mahserejian", "subjects": "Optimization and Control (math.OC)", "abstract": "In this paper, we present the mathematical formulation of an OpenStreetMaps (OSM) based tool that compares the costs and emissions of long-haul medium and heavy-duty (M&HD) electric and diesel freight trucks, and determines the spatial distribution of added energy demand due to M&HD EVs. The optimization utilizes a combination of information on routes from OSM, utility rate design data across the United States, and freight volume data, to determine these values. In order to deal with the computational complexity of this problem, we formulate the problem as a convex optimization problem that is scalable to a large geographic area. In our analysis, we further evaluate various scenarios of utility rate design (energy charges) and EV penetration rate across different geographic regions and their impact on the operating cost and emissions of the freight trucks. Our approach determines the net emissions reduction benefits of freight electrification by considering the primary energy source in different regions. Such analysis will provide insights to policy makers in designing utility rates for electric vehicle supply equipment (EVSE) operators depending upon the specific geographic region and to electric utilities in deciding infrastructure upgrades based on the spatial distribution of the added energy demand of M&HD EVs. To showcase the results, a case study for the U.S. state of Texas is conducted."}
{"main_page": "https://arxiv.org/abs/2404.00824", "pdf": "https://arxiv.org/pdf/2404.00824", "title": "Identifying a piecewise affine signal from its nonlinear observation --  application to DNA replication analysis", "authors": "Lage Clara, Pustelnik Nelly, Audit Benjamin, Arbona Jean-Michel, Gribonval R\u00e9mi", "subjects": "Optimization and Control (math.OC)", "abstract": "DNA replication stands as one of the fundamental biological processes crucial for cellular functioning. Recent experimental developments enable the study of replication dynamics at the single-molecule level for complete genomes, facilitating a deeper understanding of its main parameters. In these new data, replication dynamics is reported by the incorporation of an exogenous chemical, whose intra-cellular concentration follows a nonlinear function. The analysis of replication traces thus gives rise to a nonlinear inverse problem, presenting a nonconvex optimization challenge. We demonstrate that under noiseless conditions, the replication dynamics can be uniquely identified by the proposed model. Computing a global solution to this optimization problem is specially challenging because of its multiple local minima. We present the DNA-inverse optimization method that is capable of finding this global solution even in the presence of noise. Comparative analysis against state-of-the-art optimization methods highlights the superior computational efficiency of our approach. DNA-inverse enables the automatic recovery of all configurations of the replication dynamics, which was not possible with previous methods."}
{"main_page": "https://arxiv.org/abs/2404.00881", "pdf": "https://arxiv.org/pdf/2404.00881", "title": "Auxiliary-Variable Adaptive Control Lyapunov Barrier Functions for  Spatio-Temporally Constrained Safety-Critical Applications", "authors": "Shuo Liu, Wei Xiao, Calin A. Belta", "subjects": "Optimization and Control (math.OC)", "abstract": "Recent work has shown that stabilizing an affine control system while optimizing a quadratic cost subject to state and control constraints can be mapped to a sequence of Quadratic Programs (QPs) using Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). One of the main challenges in this method is that the QPs could easily become infeasible under safety and spatio-temporal constraints with tight control bounds. In our own recent work, we defined Auxiliary-Variable Adaptive CBFs (AVCBFs) to improve the feasibility of the CBF-based QP, while avoiding extensive parameter tuning. In this paper, we consider spatio-temporal constraints as finite-time reachability requirements. In order to satisfy these requirements, we generalize AVCBFs to Auxiliary-Variable Adaptive Control Lyapunov Barrier Functions (AVCLBFs) that work for systems and constraints with arbitrary relative degrees. We show that our method has fewer conflicts with safety and input constraints, and outperforms the state of the art in term of adaptivity and feasibility in solving the QP. We illustrate our approach on an optimal control problem for a unicycle."}
{"main_page": "https://arxiv.org/abs/2404.00940", "pdf": "https://arxiv.org/pdf/2404.00940", "title": "Sequential Decision-Making under Uncertainty: A Robust MDPs review", "authors": "Wenfan Ou, Sheng Bi", "subjects": "Optimization and Control (math.OC)", "abstract": "This review paper provides an in-depth overview of the evolution and advancements in Robust Markov Decision Processes (RMDPs), a field of paramount importance for its role in sequential decision-making amidst uncertainty. Fueled by advances in robust optimization theory and the increasing applications of reinforcement learning techniques, RMDPs literature has been enriched extensively. The review focuses on the formulation of RMDPs, particularly ambiguity sets modeling, which is central to hedging uncertainty. The review systematically classifies the extant methodologies for RMDP formulation into three principal categories: parametric, moment-based, and discrepancy-based approaches, and comprehensively dissects them. The review further delves into the rectangular assumption, which is essential for the computational tractability of RMDPs yet noted for its potential to engender overly conservative policy outcomes. The review summarizes three popular rectangular forms and presents new proof attesting to the NP-hardness of non-rectangular RMDPs. Out of traditional RMDPs scope, the review also surveys recent efforts without conventional rectangular assumptions and burgeoning research trends within the RMDP community. These studies foster the development of more flexible and practical modeling frameworks and enhance the adaptability and performance of RMDPs in the face of uncertainty."}
{"main_page": "https://arxiv.org/abs/2404.01018", "pdf": "https://arxiv.org/pdf/2404.01018", "title": "Accelerate Solving Expensive Scheduling by Leveraging Economical  Auxiliary Tasks", "authors": "Minshuo Li, Bo Liu, Bin Xin, Liang Feng, Peng Li", "subjects": "Optimization and Control (math.OC)", "abstract": "To fully leverage the multi-task optimization paradigm for accelerating the solution of expensive scheduling problems, this study has effectively tackled three vital concerns. The primary issue is identifying auxiliary tasks that closely resemble the original expensive task. We suggested a sampling strategy based on job importance, creating a compact matrix by extracting crucial rows from the entire problem specification matrix of the expensive task. This matrix serves as an economical auxiliary task. Mathematically, we proved that this economical auxiliary task bears similarity to its corresponding expensive task. The subsequent concern revolves around making auxiliary tasks more cost-effective. We determined the sampling proportions for the entire problem specification matrix through factorial design experiments, resulting in a more compact auxiliary task. With a reduced search space and shorter function evaluation time, it can rapidly furnish high-quality transferable information for the primary task. The last aspect involves designing transferable deep information from auxiliary tasks. We regarded the job priorities in the (sub-) optimal solutions to the economical auxiliary task as transferable invariants. By adopting a partial solution patching strategy, we augmented specificity knowledge onto the common knowledge to adapt to the target expensive task. The strategies devised for constructing task pairs and facilitating knowledge transfer, when incorporated into various evolutionary multitasking algorithms, were utilized to address expensive instances of permutation flow shop scheduling. Extensive experiments and statistical comparisons have validated that, with the collaborative synergy of these strategies, the performance of evolutionary multitasking algorithms is significantly enhanced in handling expensive scheduling tasks."}
{"main_page": "https://arxiv.org/abs/2404.01167", "pdf": "https://arxiv.org/pdf/2404.01167", "title": "Multiple Joint Chance Constraints Approximation for Uncertainty Modeling  in Dispatch Problems", "authors": "Yilin Wen, Yi Guo, Zechun Hu, Gabriela Hug", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "Uncertainty modeling has become increasingly important in power system decision-making. The widely-used tractable uncertainty modeling method-chance constraints with Conditional Value at Risk (CVaR) approximation, can be overconservative and even turn an originally feasible problem into an infeasible one. This paper proposes a new approximation method for multiple joint chance constraints (JCCs) to model the uncertainty in dispatch problems, which solves the conservativeness and potential infeasibility concerns of CVaR. The proposed method is also convenient for controlling the risk levels of different JCCs, which is necessary for power system applications since different resources may be affected by varying degrees of uncertainty or have different importance to the system. We then formulate a data-driven distributionally robust chance-constrained programming model for the power system multiperiod dispatch problem and leverage the proposed approximation method to solve it. In the numerical simulations, two small general examples clearly demonstrate the superiority of the proposed method, and the results of the multiperiod dispatch problem on IEEE test cases verify its practicality."}
{"main_page": "https://arxiv.org/abs/2404.01209", "pdf": "https://arxiv.org/pdf/2404.01209", "title": "Hundreds of grocery outlets needed across the United States to achieve  walkable cities", "authors": "Drew Horton, Tom Logan, Daphne Skipper, Emily Speakman", "subjects": "Optimization and Control (math.OC)", "abstract": "The notion of the $x$-minute city is again popular in urban planning, but the practical implications of developing walkable neighborhoods have not been rigorously explored. What is the scale of the challenge that cities needing to retrofit face? Where should new stores or amenities be located? For 500 cities in the United States, we explored how many additional supermarkets would be required to achieve various levels of $x$-minute access and where new stores should be located so that this access is equally-distributed. Our method is unique because it combines a novel measure of equality with a new model that optimally locates amenities for inequality-minimizing community access. We found that 25% of the studied cities could reach 15-minute access by adding five or fewer stores, while only 10% of the cities could even achieve 5-minute average access when using neighborhood centroids as potential sites; the cities that could, on average, required more than 100 stores each. This work provides a tool for cities to use evidenced-based planning to efficiently retrofit in order to enable active transport, benefiting both the climate and their residents' health. It also highlights the major challenge facing our cities due to the existing and ongoing car-dependent urban design that renders these goals unfeasible."}
{"main_page": "https://arxiv.org/abs/2404.01255", "pdf": "https://arxiv.org/pdf/2404.01255", "title": "Gradient Methods for Scalable Multi-value Electricity Network Expansion  Planning", "authors": "Anthony Degleris, Abbas El Gamal, Ram Rajagopal", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "We consider multi-value expansion planning (MEP), a general bilevel optimization model in which a planner optimizes arbitrary functions of the dispatch outcome in the presence of a partially controllable, competitive electricity market. The MEP problem can be used to jointly plan various grid assets, such as transmission, generation, and battery storage capacities; examples include identifying grid investments that minimize emissions in the absence of a carbon tax, maximizing the profit of a portfolio of renewable investments and long-term energy contracts, or reducing price inequities between different grid stakeholders. The MEP problem, however, is in general nonconvex, making it difficult to solve exactly for large real-world systems. Therefore, we propose a fast stochastic implicit gradient-based heuristic method that scales well to large networks with many scenarios. We use a strong duality reformulation and the McCormick envelope to provide a lower bound on the performance of our algorithm via convex relaxation. We test the performance of our method on a large model of the U.S. Western Interconnect and demonstrate that it scales linearly with network size and number of scenarios and can be efficiently parallelized on large machines. We find that for medium-sized 16 hour cases, gradient descent on average finds a 5.3x lower objective value in 16.5x less time compared to a traditional reformulation-based approach solved with an interior point method. We conclude with a large example in which we jointly plan transmission, generation, and storage for a 768 hour case on 100 node system, showing that emissions penalization leads to additional 40.0% reduction in carbon intensity at an additional cost of $17.1/MWh."}
{"main_page": "https://arxiv.org/abs/2404.01259", "pdf": "https://arxiv.org/pdf/2404.01259", "title": "Dynamics and Optimization in Spatially Distributed Electrical Vehicle  Charging", "authors": "Fernando Paganini, Andres Ferragut", "subjects": "Optimization and Control (math.OC); Systems and Control (eess.SY)", "abstract": "We consider a spatially distributed demand for electrical vehicle recharging, that must be covered by a fixed set of charging stations. Arriving EVs receive feedback on transport times to each station, and waiting times at congested ones, based on which they make a selfish selection. This selection determines total arrival rates in station queues, which are represented by a fluid state; departure rates are modeled under the assumption that clients have a given sojourn time in the system. The resulting differential equation system is analyzed with tools of optimization. We characterize the equilibrium as the solution to a specific convex program, which has connections to optimal transport problems, and also with road traffic theory. In particular a price of anarchy appears with respect to a social planner's allocation. From a dynamical perspective, global convergence to equilibrium is established, with tools of Lagrange duality and Lyapunov theory. An extension of the model that makes customer demand elastic to observed delays is also presented, and analyzed with extensions of the optimization machinery. Simulations to illustrate the global behavior are presented, which also help validate the model beyond the fluid approximation."}
{"main_page": "https://arxiv.org/abs/2404.01267", "pdf": "https://arxiv.org/pdf/2404.01267", "title": "Non-asymptotic Global Convergence Rates of BFGS with Exact Line Search", "authors": "Qiujiang Jin, Ruichen Jiang, Aryan Mokhtari", "subjects": "Optimization and Control (math.OC)", "abstract": "In this paper, we explore the non-asymptotic global convergence rates of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method implemented with exact line search. Notably, due to Dixon's equivalence result, our findings are also applicable to other quasi-Newton methods in the convex Broyden class employing exact line search, such as the Davidon-Fletcher-Powell (DFP) method. Specifically, we focus on problems where the objective function is strongly convex with Lipschitz continuous gradient and Hessian. Our results hold for any initial point and any symmetric positive definite initial Hessian approximation matrix. The analysis unveils a detailed three-phase convergence process, characterized by distinct linear and superlinear rates, contingent on the iteration progress. Additionally, our theoretical findings demonstrate the trade-offs between linear and superlinear convergence rates for BFGS when we modify the initial Hessian approximation matrix, a phenomenon further corroborated by our numerical experiments."}

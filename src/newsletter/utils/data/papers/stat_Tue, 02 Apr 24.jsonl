{"main_page": "https://arxiv.org/abs/2404.00109", "pdf": "https://arxiv.org/pdf/2404.00109", "title": "Reverse stress testing via multivariate modeling with vine copulas", "authors": "Menglin Zhou, Natalia Nolde", "subjects": "Applications (stat.AP)", "abstract": "As an important tool in financial risk management, stress testing aims to evaluate the stability of financial portfolios under some potential large shocks from extreme yet plausible scenarios of risk factors. The effectiveness of a stress test crucially depends on the choice of stress scenarios. In this paper we consider a pragmatic approach to stress scenario estimation that aims to address several practical challenges in the context of real life financial portfolios of currencies from a bank. Our method utilizes a flexible multivariate modelling framework based on vine copulas."}
{"main_page": "https://arxiv.org/abs/2404.00145", "pdf": "https://arxiv.org/pdf/2404.00145", "title": "Verifying the Selected Completely at Random Assumption in  Positive-Unlabeled Learning", "authors": "Pawe\u0142 Teisseyre, Konrad Furma\u0144czyk, Jan Mielniczuk", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "The goal of positive-unlabeled (PU) learning is to train a binary classifier on the basis of training data containing positive and unlabeled instances, where unlabeled observations can belong either to the positive class or to the negative class. Modeling PU data requires certain assumptions on the labeling mechanism that describes which positive observations are assigned a label. The simplest assumption, considered in early works, is SCAR (Selected Completely at Random Assumption), according to which the propensity score function, defined as the probability of assigning a label to a positive observation, is constant. On the other hand, a much more realistic assumption is SAR (Selected at Random), which states that the propensity function solely depends on the observed feature vector. SCAR-based algorithms are much simpler and computationally much faster compared to SAR-based algorithms, which usually require challenging estimation of the propensity score. In this work, we propose a relatively simple and computationally fast test that can be used to determine whether the observed data meet the SCAR assumption. Our test is based on generating artificial labels conforming to the SCAR case, which in turn allows to mimic the distribution of the test statistic under the null hypothesis of SCAR. We justify our method theoretically. In experiments, we demonstrate that the test successfully detects various deviations from SCAR scenario and at the same time it is possible to effectively control the type I error. The proposed test can be recommended as a pre-processing step to decide which final PU algorithm to choose in cases when nature of labeling mechanism is not known."}
{"main_page": "https://arxiv.org/abs/2404.00157", "pdf": "https://arxiv.org/pdf/2404.00157", "title": "Nonparametric Estimation of the Transition Density Function for  Diffusion Processes", "authors": "Fabienne Comte, Nicolas Marie", "subjects": "Statistics Theory (math.ST)", "abstract": "We assume that we observe $N$ independent copies of a diffusion process on a time interval $[0,2T]$. For a given time $t$, we estimate the transition density $p_t(x,y)$, namely the conditional density of $X_{t + s}$ given $X_s = x$, under conditions on the diffusion coefficients ensuring that this quantity exists. We use a least squares projection method on a product of finite dimensional spaces, prove risk bounds for the estimator and propose an anisotropic model selection method, relying on several reference norms. A simulation study illustrates the theoretical part for Ornstein-Uhlenbeck or square-root (Cox-Ingersoll-Ross) processes."}
{"main_page": "https://arxiv.org/abs/2404.00218", "pdf": "https://arxiv.org/pdf/2404.00218", "title": "Functional-Edged Network Modeling", "authors": "Haijie Xu, Chen Zhang", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro system data from Hong Kong and Singapore."}
{"main_page": "https://arxiv.org/abs/2404.00220", "pdf": "https://arxiv.org/pdf/2404.00220", "title": "Partially-Observable Sequential Change-Point Detection for  Autocorrelated Data via Upper Confidence Region", "authors": "Haijie Xu, Xiaochen Xian, Chen Zhang, Kaibo Liu", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Sequential change point detection for multivariate autocorrelated data is a very common problem in practice. However, when the sensing resources are limited, only a subset of variables from the multivariate system can be observed at each sensing time point. This raises the problem of partially observable multi-sensor sequential change point detection. For it, we propose a detection scheme called adaptive upper confidence region with state space model (AUCRSS). It models multivariate time series via a state space model (SSM), and uses an adaptive sampling policy for efficient change point detection and localization. A partially-observable Kalman filter algorithm is developed for online inference of SSM, and accordingly, a change point detection scheme based on a generalized likelihood ratio test is developed. How its detection power relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating the detection power as a reward, its connection with the online combinatorial multi-armed bandit (CMAB) problem is formulated and an adaptive upper confidence region algorithm is proposed for adaptive sampling policy design. Theoretical analysis of the asymptotic average detection delay is performed, and thorough numerical studies with synthetic data and real-world data are conducted to demonstrate the effectiveness of our method."}
{"main_page": "https://arxiv.org/abs/2404.00221", "pdf": "https://arxiv.org/pdf/2404.00221", "title": "Robust Learning for Optimal Dynamic Treatment Regimes with Observational  Data", "authors": "Shosei Sakaguchi", "subjects": "Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)", "abstract": "Many public policies and medical interventions involve dynamics in their treatment assignments, where treatments are sequentially assigned to the same individuals across multiple stages, and the effect of treatment at each stage is usually heterogeneous with respect to the history of prior treatments and associated characteristics. We study statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's history. We propose a step-wise doubly-robust approach to learn the optimal DTR using observational data under the assumption of sequential ignorability. The approach solves the sequential treatment assignment problem through backward induction, where, at each step, we combine estimators of propensity scores and action-value functions (Q-functions) to construct augmented inverse probability weighting estimators of values of policies for each stage. The approach consistently estimates the optimal DTR if either a propensity score or Q-function for each stage is consistently estimated. Furthermore, the resulting DTR can achieve the optimal convergence rate $n^{-1/2}$ of regret under mild conditions on the convergence rate for estimators of the nuisance parameters."}
{"main_page": "https://arxiv.org/abs/2404.00256", "pdf": "https://arxiv.org/pdf/2404.00256", "title": "Objective Bayesian FDR", "authors": "Yoshiko Hayashi", "subjects": "Methodology (stat.ME)", "abstract": "Here, we develop an objective Bayesian analysis for large-scale datasets. When Bayesian analysis is applied to large-scale datasets, the cut point that provides the posterior probability is usually determined following customs. In this work, we propose setting the cut point in an objective manner, which is determined so as to match the posterior null number with the estimated true null number. The posterior probability obtained using an objective cut point is relatively similar to the real false discovery rate (FDR), which facilitates control of the FDR level."}
{"main_page": "https://arxiv.org/abs/2404.00294", "pdf": "https://arxiv.org/pdf/2404.00294", "title": "Information divergences and likelihood ratios of Poisson processes and  point patterns", "authors": "Lasse Leskel\u00e4", "subjects": "Statistics Theory (math.ST); Information Theory (cs.IT); Probability (math.PR)", "abstract": "This article develops an analytical framework for studying information divergences and likelihood ratios associated with Poisson processes and point patterns on general measurable spaces. The main results include explicit analytical formulas for Kullback-Leibler divergences, R\\'enyi divergences, Hellinger distances, and likelihood ratios of the laws of Poisson point patterns in terms of their intensity measures. The general results yield similar formulas for inhomogeneous Poisson processes, compound Poisson processes, as well as spatial and marked Poisson point patterns. Additional results include simple characterisations of absolute continuity, mutual singularity, and the existence of common dominating measures. The analytical toolbox is based on Tsallis divergences of sigma-finite measures on abstract measurable spaces. The treatment is purely information-theoretic and free of any topological assumptions."}
{"main_page": "https://arxiv.org/abs/2404.00319", "pdf": "https://arxiv.org/pdf/2404.00319", "title": "Direction Preferring Confidence Intervals", "authors": "Tzviel Frostig, Yoav Benjamini, Ruth Heller", "subjects": "Methodology (stat.ME); Applications (stat.AP)", "abstract": "Confidence intervals (CIs) are instrumental in statistical analysis, providing a range estimate of the parameters. In modern statistics, selective inference is common, where only certain parameters are highlighted. However, this selective approach can bias the inference, leading some to advocate for the use of CIs over p-values. To increase the flexibility of confidence intervals, we introduce direction-preferring CIs, enabling analysts to focus on parameters trending in a particular direction. We present these types of CIs in two settings: First, when there is no selection of parameters; and second, for situations involving parameter selection, where we offer a conditional version of the direction-preferring CIs. Both of these methods build upon the foundations of Modified Pratt CIs, which rely on non-equivariant acceptance regions to achieve longer intervals in exchange for improved sign exclusions. We show that for selected parameters out of m > 1 initial parameters of interest, CIs aimed at controlling the false coverage rate, have higher power to determine the sign compared to conditional CIs. We also show that conditional confidence intervals control the marginal false coverage rate (mFCR) under any dependency."}
{"main_page": "https://arxiv.org/abs/2404.00359", "pdf": "https://arxiv.org/pdf/2404.00359", "title": "Loss-based prior for tree topologies in BART models", "authors": "F. Serafini, F. Leisen, C. Villa, K. Wilson", "subjects": "Methodology (stat.ME); Applications (stat.AP)", "abstract": "We present a novel prior for tree topology within Bayesian Additive Regression Trees (BART) models. This approach quantifies the hypothetical loss in information and the loss due to complexity associated with choosing the wrong tree structure. The resulting prior distribution is compellingly geared toward sparsity, a critical feature considering BART models' tendency to overfit. Our method incorporates prior knowledge into the distribution via two parameters that govern the tree's depth and balance between its left and right branches. Additionally, we propose a default calibration for these parameters, offering an objective version of the prior. We demonstrate our method's efficacy on both simulated and real datasets."}
{"main_page": "https://arxiv.org/abs/2404.00398", "pdf": "https://arxiv.org/pdf/2404.00398", "title": "Revisiting the region determined by Spearman's $\u03c1$ and Spearman's  footrule $\u03c6$", "authors": "Marco Tschimpke, Manuela Schreyer, Wolfgang Trutschnig", "subjects": "Statistics Theory (math.ST)", "abstract": "Kokol and Stopar ($2023$) recently studied the exact region $\\Omega_{\\phi,\\rho}$ determined by Spearman's footrule $\\phi$ and Spearman's $\\rho$ and derived a sharp lower, as well as a non-sharp upper bound for $\\rho$ given $\\phi$. Considering that the proofs for establishing these inequalities are novel and interesting, but technically quite involved we here provide alternative simpler proofs mainly building upon shuffles, symmetry, denseness and mass shifting. As a by-product of these proofs we derive several additional results on shuffle rearrangements and the interplay between diagonal copulas and shuffles which are of independent interest. Moreover we finally show that we can get closer to the (non-sharp) upper bound than established in the literature so far."}
{"main_page": "https://arxiv.org/abs/2404.00481", "pdf": "https://arxiv.org/pdf/2404.00481", "title": "Convolutional Bayesian Filtering", "authors": "Wenhan Cao, Shiqi Liu, Chang Liu, Zeyu He, Stephen S.-T. Yau, Shengbo Eben Li", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Systems and Control (eess.SY)", "abstract": "Bayesian filtering serves as the mainstream framework of state estimation in dynamic systems. Its standard version utilizes total probability rule and Bayes' law alternatively, where how to define and compute conditional probability is critical to state distribution inference. Previously, the conditional probability is assumed to be exactly known, which represents a measure of the occurrence probability of one event, given the second event. In this paper, we find that by adding an additional event that stipulates an inequality condition, we can transform the conditional probability into a special integration that is analogous to convolution. Based on this transformation, we show that both transition probability and output probability can be generalized to convolutional forms, resulting in a more general filtering framework that we call convolutional Bayesian filtering. This new framework encompasses standard Bayesian filtering as a special case when the distance metric of the inequality condition is selected as Dirac delta function. It also allows for a more nuanced consideration of model mismatch by choosing different types of inequality conditions. For instance, when the distance metric is defined in a distributional sense, the transition probability and output probability can be approximated by simply rescaling them into fractional powers. Under this framework, a robust version of Kalman filter can be constructed by only altering the noise covariance matrix, while maintaining the conjugate nature of Gaussian distributions. Finally, we exemplify the effectiveness of our approach by reshaping classic filtering algorithms into convolutional versions, including Kalman filter, extended Kalman filter, unscented Kalman filter and particle filter."}
{"main_page": "https://arxiv.org/abs/2404.00551", "pdf": "https://arxiv.org/pdf/2404.00551", "title": "Convergence of Continuous Normalizing Flows for Learning Probability  Distributions", "authors": "Yuan Gao, Jian Huang, Yuling Jiao, Shurong Zheng", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the error due to velocity estimation, the discretization error, and the early stopping error. A key step in our analysis involves establishing the regularity properties of the velocity field and its estimator for CNFs constructed with linear interpolation. This necessitates the development of uniform error bounds with Lipschitz regularity control of deep ReLU networks that approximate the Lipschitz function class, which could be of independent interest. Our nonparametric convergence analysis offers theoretical guarantees for using CNFs to learn probability distributions from a finite random sample."}
{"main_page": "https://arxiv.org/abs/2404.00606", "pdf": "https://arxiv.org/pdf/2404.00606", "title": "\"Sound and Fury\": Nonlinear Functionals of Volatility Matrix in the  Presence of Jump and Noise", "authors": "Richard Y. Chen", "subjects": "Methodology (stat.ME)", "abstract": "This paper resolves a pivotal open problem on nonparametric inference for nonlinear functionals of volatility matrix. Multiple prominent statistical tasks can be formulated as functionals of volatility matrix, yet a unified statistical theory of general nonlinear functionals based on noisy data remains challenging and elusive. Nonetheless, this paper shows it can be achieved by combining the strengths of pre-averaging, jump truncation and nonlinearity bias correction. In light of general nonlinearity, bias correction beyond linear approximation becomes necessary. Resultant estimators are nonparametric and robust over a wide spectrum of stochastic models. Moreover, the estimators can be rate-optimal and stable central limit theorems are obtained. The proposed framework lends itself conveniently to uncertainty quantification and permits fully feasible inference. With strong theoretical guarantees, this paper provides an inferential foundation for a wealth of statistical methods for noisy high-frequency data, such as realized principal component analysis, continuous-time linear regression, realized Laplace transform, generalized method of integrated moments and specification tests, hence extends current application scopes to noisy data which is more prevalent in practice."}
{"main_page": "https://arxiv.org/abs/2404.00630", "pdf": "https://arxiv.org/pdf/2404.00630", "title": "Sobolev Calibration of Imperfect Computer Models", "authors": "Qingwen Zhang, Wenjia Wang", "subjects": "Statistics Theory (math.ST)", "abstract": "Calibration refers to the statistical estimation of unknown model parameters in computer experiments, such that computer experiments can match underlying physical systems. This work develops a new calibration method for imperfect computer models, Sobolev calibration, which can rule out calibration parameters that generate overfitting calibrated functions. We prove that the Sobolev calibration enjoys desired theoretical properties including fast convergence rate, asymptotic normality and semiparametric efficiency. We also demonstrate an interesting property that the Sobolev calibration can bridge the gap between two influential methods: $L_2$ calibration and Kennedy and O'Hagan's calibration. In addition to exploring the deterministic physical experiments, we theoretically justify that our method can transfer to the case when the physical process is indeed a Gaussian process, which follows the original idea of Kennedy and O'Hagan's. Numerical simulations as well as a real-world example illustrate the competitive performance of the proposed method."}
{"main_page": "https://arxiv.org/abs/2404.00735", "pdf": "https://arxiv.org/pdf/2404.00735", "title": "Two-Stage Nuisance Function Estimation for Causal Mediation Analysis", "authors": "AmirEmad Ghassami", "subjects": "Methodology (stat.ME); Machine Learning (stat.ML)", "abstract": "When estimating the direct and indirect causal effects using the influence function-based estimator of the mediation functional, it is crucial to understand what aspects of the treatment, the mediator, and the outcome mean mechanisms should be focused on. Specifically, considering them as nuisance functions and attempting to fit these nuisance functions as accurate as possible is not necessarily the best approach to take. In this work, we propose a two-stage estimation strategy for the nuisance functions that estimates the nuisance functions based on the role they play in the structure of the bias of the influence function-based estimator of the mediation functional. We provide robustness analysis of the proposed method, as well as sufficient conditions for consistency and asymptotic normality of the estimator of the parameter of interest."}
{"main_page": "https://arxiv.org/abs/2404.00751", "pdf": "https://arxiv.org/pdf/2404.00751", "title": "C-XGBoost: A tree boosting model for causal effect estimation", "authors": "Niki Kiriakidou, Ioannis E. Livieris, Christos Diou", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)", "abstract": "Causal effect estimation aims at estimating the Average Treatment Effect as well as the Conditional Average Treatment Effect of a treatment to an outcome from the available data. This knowledge is important in many safety-critical domains, where it often needs to be extracted from observational data. In this work, we propose a new causal inference model, named C-XGBoost, for the prediction of potential outcomes. The motivation of our approach is to exploit the superiority of tree-based models for handling tabular data together with the notable property of causal inference neural network-based models to learn representations that are useful for estimating the outcome for both the treatment and non-treatment cases. The proposed model also inherits the considerable advantages of XGBoost model such as efficiently handling features with missing values requiring minimum preprocessing effort, as well as it is equipped with regularization techniques to avoid overfitting/bias. Furthermore, we propose a new loss function for efficiently training the proposed causal inference model. The experimental analysis, which is based on the performance profiles of Dolan and Mor{\\'e} as well as on post-hoc and non-parametric statistical tests, provide strong evidence about the effectiveness of the proposed approach."}
{"main_page": "https://arxiv.org/abs/2404.00753", "pdf": "https://arxiv.org/pdf/2404.00753", "title": "A compromise criterion for weighted least squares estimates", "authors": "Jordan Bryan, Haibo Zhou, Didong Li", "subjects": "Statistics Theory (math.ST); Methodology (stat.ME)", "abstract": "When independent errors in a linear model have non-identity covariance, the ordinary least squares estimate of the model coefficients is less efficient than the weighted least squares estimate. However, the practical application of weighted least squares is challenging due to its reliance on the unknown error covariance matrix. Although feasible weighted least squares estimates, which use an approximation of this matrix, often outperform the ordinary least squares estimate in terms of efficiency, this is not always the case. In some situations, feasible weighted least squares can be less efficient than ordinary least squares. This study identifies the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how a certain robust regression estimate behaves with respect to the linear model with normal errors of unequal variance."}
{"main_page": "https://arxiv.org/abs/2404.00784", "pdf": "https://arxiv.org/pdf/2404.00784", "title": "Estimating sample paths of Gauss-Markov processes from noisy data", "authors": "Benjamin Davies", "subjects": "Statistics Theory (math.ST); Econometrics (econ.EM); Probability (math.PR)", "abstract": "I derive the pointwise conditional means and variances of an arbitrary Gauss-Markov process, given noisy observations of points on a sample path. These moments depend on the process's mean and covariance functions, and on the conditional moments of the sampled points. I study the Brownian motion and bridge as special cases."}
{"main_page": "https://arxiv.org/abs/2404.00788", "pdf": "https://arxiv.org/pdf/2404.00788", "title": "A Novel Stratified Analysis Method for Testing and Estimating Overall  Treatment Effects on Time-to-Event Outcomes Using Average Hazard with  Survival Weight", "authors": "Zihan Qian, Lu Tian, Miki Horiguchi, Hajime Uno", "subjects": "Methodology (stat.ME)", "abstract": "Given the limitations of using the Cox hazard ratio to summarize the magnitude of the treatment effect, alternative measures that do not have these limitations are gaining attention. One of the recently proposed alternative methods uses the average hazard with survival weight (AH). This population quantity can be interpreted as the average intensity of the event occurrence in a given time window that does not involve study-specific censoring. Inference procedures for the ratio of AH and difference in AH have already been proposed in simple randomized controlled trial settings to compare two groups. However, methods with stratification factors have not been well discussed, although stratified analysis is often used in practice to adjust for confounding factors and increase the power to detect a between-group difference. The conventional stratified analysis or meta-analysis approach, which integrates stratum-specific treatment effects using an optimal weight, directly applies to the ratio of AH and difference in AH. However, this conventional approach has significant limitations similar to the Cochran-Mantel-Haenszel method for a binary outcome and the stratified Cox procedure for a time-to-event outcome. To address this, we propose a new stratified analysis method for AH using standardization. With the proposed method, one can summarize the between-group treatment effect in both absolute difference and relative terms, adjusting for stratification factors. This can be a valuable alternative to the traditional stratified Cox procedure to estimate and report the magnitude of the treatment effect on time-to-event outcomes using hazard."}
{"main_page": "https://arxiv.org/abs/2404.00820", "pdf": "https://arxiv.org/pdf/2404.00820", "title": "Visual analysis of bivariate dependence between continuous random  variables", "authors": "Arturo Erdely, Manuel Rubio-Sanchez", "subjects": "Methodology (stat.ME)", "abstract": "Scatter plots are widely recognized as fundamental tools for illustrating the relationship between two numerical variables. Despite this, based on solid theoretical foundations, scatter plots generated from pairs of continuous random variables may not serve as reliable tools for assessing dependence. Sklar's Theorem implies that scatter plots created from ranked data are preferable for such analysis as they exclusively convey information pertinent to dependence. This is in stark contrast to conventional scatter plots, which also encapsulate information about the variables' marginal distributions. Such additional information is extraneous to dependence analysis and can obscure the visual interpretation of the variables' relationship. In this article, we delve into the theoretical underpinnings of these ranked data scatter plots, hereafter referred to as rank plots. We offer insights into interpreting the information they reveal and examine their connections with various association measures, including Pearson's and Spearman's correlation coefficients, as well as Schweizer-Wolff's measure of dependence. Furthermore, we introduce a novel graphical combination for dependence analysis, termed a dplot, and demonstrate its efficacy through real data examples."}
{"main_page": "https://arxiv.org/abs/2404.00888", "pdf": "https://arxiv.org/pdf/2404.00888", "title": "Two step estimations via the Dantzig selector for models of stochastic  processes with high-dimensional parameters", "authors": "Kou Fujimori, Koji Tsukuda", "subjects": "Statistics Theory (math.ST)", "abstract": "We consider the sparse estimation for stochastic processes with possibly infinite-dimensional nuisance parameters, by using the Dantzig selector which is a sparse estimation method similar to $Z$-estimation. When a consistent estimator for a nuisance parameter is obtained, it is possible to construct an asymptotically normal estimator for the parameter of interest under appropriate conditions. Motivated by this fact, we establish the asymptotic behavior of the Dantzig selector for models of ergodic stochastic processes with high-dimensional parameters of interest and possibly infinite-dimensional nuisance parameters. Applications to ergodic time series models including integer-valued autoregressive models and ergodic diffusion processes are presented."}
{"main_page": "https://arxiv.org/abs/2404.00912", "pdf": "https://arxiv.org/pdf/2404.00912", "title": "Inference in Randomized Least Squares and PCA via Normality of Quadratic  Forms", "authors": "Leda Wang, Zhixiang Zhang, Edgar Dobriban", "subjects": "Statistics Theory (math.ST); Computation (stat.CO); Methodology (stat.ME); Machine Learning (stat.ML)", "abstract": "Randomized algorithms can be used to speed up the analysis of large datasets. In this paper, we develop a unified methodology for statistical inference via randomized sketching or projections in two of the most fundamental problems in multivariate statistical analysis: least squares and PCA. The methodology applies to fixed datasets -- i.e., is data-conditional -- and the only randomness is due to the randomized algorithm. We propose statistical inference methods for a broad range of sketching distributions, such as the subsampled randomized Hadamard transform (SRHT), Sparse Sign Embeddings (SSE) and CountSketch, sketching matrices with i.i.d. entries, and uniform subsampling. To our knowledge, no comparable methods are available for SSE and for SRHT in PCA. Our novel theoretical approach rests on showing the asymptotic normality of certain quadratic forms. As a contribution of broader interest, we show central limit theorems for quadratic forms of the SRHT, relying on a novel proof via a dyadic expansion that leverages the recursive structure of the Hadamard transform. Numerical experiments using both synthetic and empirical datasets support the efficacy of our methods, and in particular suggest that sketching methods can have better computation-estimation tradeoffs than recently proposed optimal subsampling methods."}
{"main_page": "https://arxiv.org/abs/2404.01043", "pdf": "https://arxiv.org/pdf/2404.01043", "title": "The Mean Shape under the Relative Curvature Condition", "authors": "Mohsen Taheri, Stephen M. Pizer, J\u00f6rn Schulz", "subjects": "Methodology (stat.ME)", "abstract": "The relative curvature condition (RCC) serves as a crucial constraint, ensuring the avoidance of self-intersection problems in calculating the mean shape over a sample of swept regions. By considering the RCC, this work discusses estimating the mean shape for a class of swept regions called elliptical slabular objects based on a novel shape representation, namely elliptical tube representation (ETRep). The ETRep shape space equipped with extrinsic and intrinsic distances in accordance with object transformation is explained. The intrinsic distance is determined based on the intrinsic skeletal coordinate system of the shape space. Further, calculating the intrinsic mean shape based on the intrinsic distance over a set of ETReps is demonstrated. The proposed intrinsic methodology is applied for the statistical shape analysis to design global and partial hypothesis testing methods to study the hippocampal structure in early Parkinson's disease."}
{"main_page": "https://arxiv.org/abs/2404.01076", "pdf": "https://arxiv.org/pdf/2404.01076", "title": "Debiased calibration estimation using generalized entropy in survey  sampling", "authors": "Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu", "subjects": "Methodology (stat.ME)", "abstract": "Incorporating the auxiliary information into the survey estimation is a fundamental problem in survey sampling. Calibration weighting is a popular tool for incorporating the auxiliary information. The calibration weighting method of Deville and Sarndal (1992) uses a distance measure between the design weights and the final weights to solve the optimization problem with calibration constraints. This paper introduces a novel framework that leverages generalized entropy as the objective function for optimization, where design weights play a role in the constraints to ensure design consistency, rather than being part of the objective function. This innovative calibration framework is particularly attractive due to its generality and its ability to generate more efficient calibration weights compared to traditional methods based on Deville and Sarndal (1992). Furthermore, we identify the optimal choice of the generalized entropy function that achieves the minimum variance across various choices of the generalized entropy function under the same constraints. Asymptotic properties, such as design consistency and asymptotic normality, are presented rigorously. The results from a limited simulation study are also presented. We demonstrate a real-life application using agricultural survey data collected from Kynetec, Inc."}
{"main_page": "https://arxiv.org/abs/2404.01153", "pdf": "https://arxiv.org/pdf/2404.01153", "title": "TransFusion: Covariate-Shift Robust Transfer Learning for  High-Dimensional Regression", "authors": "Zelin He, Ying Sun, Jingyuan Liu, Runze Li", "subjects": "Machine Learning (stat.ML); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)", "abstract": "The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method's robustness to covariate shifts."}
{"main_page": "https://arxiv.org/abs/2404.01191", "pdf": "https://arxiv.org/pdf/2404.01191", "title": "A Semiparametric Approach for Robust and Efficient Learning with Biobank  Data", "authors": "Molei Liu, Xinyi Wang, Chuan Hong", "subjects": "Methodology (stat.ME)", "abstract": "With the increasing availability of electronic health records (EHR) linked with biobank data for translational research, a critical step in realizing its potential is to accurately classify phenotypes for patients. Existing approaches to achieve this goal are based on error-prone EHR surrogate outcomes, assisted and validated by a small set of labels obtained via medical chart review, which may also be subject to misclassification. Ignoring the noise in these outcomes can induce severe estimation and validation bias to both EHR phenotyping and risking modeling with biomarkers collected in the biobank. To overcome this challenge, we propose a novel unsupervised and semiparametric approach to jointly model multiple noisy EHR outcomes with their linked biobank features. Our approach primarily aims at disease risk modeling with the baseline biomarkers, and is also able to produce a predictive EHR phenotyping model and validate its performance without observations of the true disease outcome. It consists of composite and nonparametric regression steps free of any parametric model specification, followed by a parametric projection step to reduce the uncertainty and improve the estimation efficiency. We show that our method is robust to violations of the parametric assumptions while attaining the desirable root-$n$ convergence rates on risk modeling. Our developed method outperforms existing methods in extensive simulation studies, as well as a real-world application in phenotyping and genetic risk modeling of type II diabetes."}
{"main_page": "https://arxiv.org/abs/2404.01200", "pdf": "https://arxiv.org/pdf/2404.01200", "title": "Large-Scale Non-convex Stochastic Constrained Distributionally Robust  Optimization", "authors": "Qi Zhang, Yi Zhou, Ashley Prater-Bennette, Lixin Shen, Shaofeng Zou", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\\chi^2$-divergences as a special case. We prove that our algorithm finds an $\\epsilon$-stationary point with a computational complexity of $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the Cressie-Read divergence. The numerical results indicate that our method outperforms existing methods.} Our method also applies to the smoothed conditional value at risk (CVaR) DRO."}
{"main_page": "https://arxiv.org/abs/2404.01233", "pdf": "https://arxiv.org/pdf/2404.01233", "title": "Optimal Ridge Regularization for Out-of-Distribution Prediction", "authors": "Pratik Patil, Jin-Hong Du, Ryan J. Tibshirani", "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the train or the test distributions, except for moment bounds, and allow for arbitrary shifts and the widest possible range of (negative) regularization levels."}
{"main_page": "https://arxiv.org/abs/2404.01245", "pdf": "https://arxiv.org/pdf/2404.01245", "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,  Detection Efficiency and Optimal Rules", "authors": "Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su", "subjects": "Statistics Theory (math.ST); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments."}
